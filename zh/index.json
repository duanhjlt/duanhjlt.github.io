[{"content":"周末注册了不少的Profile类型的网站，这些网站都得自己找地方展示，现在集中放到这里吧，每个都是一个外链啊，就是不知道有没有。\nRamen.Tools Bento Startup Fame MAKE.RS DEV ONE PAGE BIO.SITES Tap Bio Folllio LinkTree PeerList TwitterXVideo | FrogDR Brat-Gen | FrogDR Cal.app Twitch ollama ","permalink":"https://blog.crazykids.tech/zh/posts/backlink_profile/","summary":"周末注册了不少的Profile类型的网站，这些网站都得自己找地方展示，现在集中放到这里吧，每个都是一个外链啊，就是不知道有没有。 Ramen.Tools Bento Startup Fame MAKE.RS DEV ONE PAGE BIO.SITES Tap Bio Folllio LinkTree PeerList TwitterXVideo | FrogDR Brat-Gen | FrogDR Cal.app Twitch ollama","title":"Profile集合"},{"content":"引言：数字内容时代的挑战与机遇 在社交媒体已经渗透到生活方方面面的今天，Twitter（现更名为X）以其即时、高密度的信息流，成为了新闻事件、行业动态、流行文化和个人观点的核心集散地。对于内容创作者、社交媒体经理、记者乃至普通用户而言，这个平台上的海量视频、图片和文字既是宝贵的素材库，也是一个充满挑战的生态系统。\n挑战在于，平台原生功能并不支持内容的便捷下载与再利用。当您看到一个富有洞见的视频、一张极具冲击力的图片或一段需要跨语言理解的推文时，如何高效、完整地将其保存下来，并进行后续的编辑、翻译或再发布，就成了一个亟待解决的痛点。正是基于此需求，各类Twitter/X下载工具应运而生。然而，当市场从“有没有”进入“好不好”的阶段时，一款工具的价值，已远不止于满足单一的下载功能。本文将深入探讨，以TwitterXVideo为例，一款顶级的现代内容工具是如何超越其竞争者（如TwitterXZ、TWMate），从一个简单的“下载器”进化为功能强大的“内容工作站”。\n评测框架：如何定义一款卓越的Twitter/X内容工具？ 要进行有意义的比较，我们必须建立一个全面的评测框架，它应涵盖从基础功能到高阶应用，再到用户体验的每一个环节。\n基础核心能力 (Core Functionality): 这是工具的立身之本。\n媒体格式支持度： 是否支持视频、GIF、图片乃至音频（MP3）的全格式下载？\n下载质量与速度： 能否提供不同分辨率的选项？在网络环境良好时，下载速度是否稳定、迅速？\n批量处理能力： 能否一键解析并下载某个用户时间线上的所有媒体？这是衡量效率的关键指标。\n高级扩展功能 (Advanced Features): 这是工具拉开差距的关键。\n内容再创作支持： 是否内建了文本编辑、翻译等功能，以支持内容的二次加工？\n自动化与集成： 能否通过API等方式与其他平台或账户联动，实现自动化流程，如自动转发？\n用户体验与设计 (Usability \u0026amp; UX): 这决定了用户的留存率。\n操作便捷性： 是否需要繁琐的注册登录流程？界面设计是否直观、易于上手？\n纯净度与干扰： 应用中是否存在大量干扰性广告？整体操作流程是否顺畅无卡顿？\n成本模型： 是完全免费，还是有隐藏的付费墙或功能限制？\n市场主流工具横向评测：从“能用”到“好用”的差距 我们将以TwitterXVideo、TwitterXZ和TWMate这三款工具为例，在上述框架下进行横向对比。\n第一轮：核心下载能力对决\nTwitterXZ \u0026amp; TWMate： 这两款工具在基础下载方面表现尚可。它们都能解决“如何下载Twitter视频”这一核心问题。用户只需复制推文链接，粘贴到输入框，即可解析出下载地址。TwitterXZ的支持格式相对较全，涵盖视频、图片、GIF和MP3。而TWMate则主要聚焦于视频和GIF，功能较为单一。在批量下载方面，这两款工具均未明确提供便捷的用户媒体批量下载功能，通常需要用户逐条复制链接，效率较低。\nTwitterXVideo： 在基础层面，TwitterXVideo做到了全面覆盖，支持视频（多种分辨率）、图片、GIF及MP3的下载，确保用户能够获取所需的一切媒体格式。其最大的亮点在于支持通过用户搜索进行批量媒体下载。这意味着，您只需输入特定用户的ID，就能一次性解析该用户发布过的所有公开媒体内容，这对于需要备份或分析特定账户内容的用户来说，是革命性的效率提升。\n第二轮：用户体验与成本考量\n在这一点上，三者都采取了无需注册的便捷模式，降低了使用门槛。然而，差异体现在细节中。\nTwitterXZ \u0026amp; TWMate： 为了维持运营，这两款工具的界面中都嵌入了数量不等的广告。虽然可以理解，但这些弹窗或横幅广告在一定程度上会干扰操作流程，影响整体的流畅体验。\nTwitterXVideo： 其作出了一个极为大胆的承诺——完全免费且无任何广告。这提供了一种堪称“奢侈”的纯净体验。用户在整个使用过程中，不会被任何无关信息打扰，所有注意力都可以集中在内容处理本身，这在免费工具中是极为罕见的。\n深度聚焦：TwitterXVideo的“杀手级”功能矩阵——从“工具”到“工作站”的进化 如果说无广告和批量下载让TwitterXVideo成为了一款“优秀”的下载器，那么其独创的整合功能，则使其彻底升维，成为一个“强大”的内容工作站。它完美地诠释了“1+1+1 \u0026gt; 3”的整合效应。\n一键编辑：内容再创作的起点 在解析推文后，TwitterXVideo并非简单地提供下载链接。它内建了一个简洁的文本编辑器。用户可以直接在界面中修改原始推文的文字内容。这个看似微小的功能，其实用场景非常广泛：\n修正与补充： 为转发的内容添加自己的评论或更正信息。\n跨平台发布： 调整措辞和格式，以适应微博、Facebook或Instagram等不同平台的发布要求。\n内容存档： 在保存视频或图片的同时，用更准确的描述来备注内容，方便日后检索。\n一键翻译：跨越语言的信息壁垒 面对全球化的信息流，语言是获取信息的最大障碍之一。TwitterXVideo集成了一键翻译功能，可以即时将外语推文翻译成用户的目标语言。这使得用户能够：\n快速理解全球动态： 无需借助第三方翻译软件，即可迅速掌握来自不同国家和地区的新闻与观点。\n提升信息处理效率： 在研究或收集资料时，能够无障碍地处理多语言信源，极大地拓宽了信息获取的广度。\nAPI自动转发：终极效率武器 这是TwitterXVideo最具颠覆性的“决定性功能”。通过安全地集成Twitter API，它实现了从下载到再发布的无缝闭环操作。用户在完成下载、编辑和翻译后，可以点击一个按钮，将处理好的内容（包括媒体和文字）自动发布到自己指定的Twitter/X账户。\n对内容创作者而言： 这是巨大的福音。他们可以快速地将收集到的优质素材进行二次创作，并立即分享给自己的粉丝，整个过程无需在多个应用或浏览器标签页之间来回切换。\n对社交媒体经理而言： 这极大地优化了工作流程。以往需要“下载视频 -\u0026gt; 保存到本地 -\u0026gt; 打开自己账户 -\u0026gt; 上传视频 -\u0026gt; 撰写文案 -\u0026gt; 发布”的繁琐步骤，现在被压缩为在同一个界面内的几次点击。这不仅节省了海量时间，也降低了手动操作中可能出现的失误。\n结论：选择工具，就是选择一种工作方式 回到最初的问题：哪款工具是最好的？答案取决于您的需求层次。\n对于临时、基础的用户： 如果您只是偶尔需要下载一个视频离线观看，那么TwitterXZ或TWMate这类简单的工具已经足够满足您的需求。\n对于高频、专业的用户： 如果您是一位内容创作者、社交媒体运营者、新闻记者或市场研究员，您的需求早已超越了“下载”。您需要的是效率、整合与流畅的工作流。在这种情况下，TwitterXVideo提供的价值是其他竞争者无法比拟的。\n它不仅仅是一个免费、无广告的下载器，更是一个集“获取（批量下载）- 处理（编辑、翻译）- 分发（API自动转发）”于一体的综合性内容管理平台。它通过前瞻性的功能整合，将原本分散、耗时的工作环节串联起来，为用户提供了前所未有的便利性和效率。因此，从功能深度、用户体验和工作流重塑的角度来看，TwitterXVideo无疑是当今市场上最强大、最全面的Twitter/X内容解决方案。它代表的，是从单一工具到高效工作站的进化方向。\n","permalink":"https://blog.crazykids.tech/zh/posts/how-twitterxvideo-redefines-workflows/","summary":"引言：数字内容时代的挑战与机遇 在社交媒体已经渗透到生活方方面面的今天，Twitter（现更名为X）以其即时、高密度的信息流，成为了新闻事件、行业动态、流行文化和个人观点的核心集散地。对于内容创作者、社交媒体经理、记者乃至普通用户而言，这个平台上的海量视频、图片和文字既是宝贵的素材","title":"深度剖析：在Twitter/X内容工具的红海中，TwitterXVideo何以重塑工作流？"},{"content":" 原文：Reflections on OpenAI\n三周前，我离开了OpenAI。我是在2024年5月加入这家公司的。\n我想分享一些我的思考，因为外界对OpenAI的所作所为众说纷纭、云里雾里，但很少有关于在那里工作的真实文化感受的第一手描述。\nNabeel Qureshi写过一篇很棒的文章，叫做《对Palantir的反思》，文中他深入思考了是什么让Palantir与众不同。我也想趁着记忆犹新，为OpenAI做同样的事情。你在这里不会看到任何商业机密，更多的是关于这个史上最引人入胜的组织之一，在当前这个极其有趣的时刻，我对其的一些反思。\n首先要说明的是：我决定离开并没有任何个人恩怨——事实上，我对此内心非常纠结。从一个创业公司的创始人转变为一个3000人组织的员工，这个过程是艰难的。现在，我渴望一个新的开始。\n工作的质量很有可能会吸引我再次回去。很难想象还有什么比构建AGI（通用人工智能）更具影响力的事了，而LLM（大语言模型）无疑是这十年来的技术创新之最。我很幸运能亲眼见证一些发展，并参与了Codex的发布。\n当然，这些只是我个人的观察，并不代表公司的观点。OpenAI是一个很大的地方，这只是我管中窥豹，所见有限。\n文化 关于OpenAI，首先要知道的是它发展得有多快。我刚加入时，公司只有1000多人。一年后，已经超过3000人，而我的司龄已经能排进前30%。几乎所有的领导层，他们现在的工作都和两三年前大相径庭。¹\n当然，当你扩张得这么快时，一切都会出问题：公司层面的沟通方式、汇报结构、产品交付流程、人员管理和组织方式、招聘流程等等。不同团队的文化差异很大：有些团队总是在全力冲刺，有些团队则是在照看大型的训练任务，还有些则以一种更为稳健的节奏前进。不存在单一的OpenAI体验，研究、应用和GTM（市场推广）团队的运作时间维度也截然不同。\nOpenAI一个不寻常之处在于，所有事情——我是说所有事情——都在Slack上进行。没有电子邮件。我在那里的全部时间里，可能总共就收了大约10封邮件。如果你不善于组织信息，你会觉得这极其分散注意力。但如果你用心管理你的频道和通知，还是能让工作变得相当高效的。\nOpenAI的文化非常自下而上，尤其是在研究领域。我刚来的时候，开始询问下一季度的路线图。我得到的回答是：“这东西不存在”（不过现在有了）。好的想法可以来自任何地方，而且事先往往很难判断哪个想法会最有成果。与其说有一个宏大的“总体规划”，不如说进展是迭代式的，是随着新研究成果的出现而逐步揭示的。\n得益于这种自下而上的文化，OpenAI也非常任人唯贤。从历史上看，公司领导的晋升主要基于他们提出好点子并执行的能力。许多能力极强的领导并不擅长在全体会议上演讲或玩弄政治手腕。在OpenAI，这些事没有在其他公司那么重要。最好的想法往往能最终胜出。²\n这里有强烈的行动偏好（你可以直接动手去做）。不同但相关的团队最终想到一块儿去，这种情况并不少见。我最初参与了一个与ChatGPT Connectors类似（但对内）的项目。在我们决定推动发布之前，公司内部大概有三到四个不同的Codex原型在流传。这些项目通常是由少数几个人在没有请求许可的情况下发起的。一旦它们显示出潜力，团队就会迅速围绕它们形成。\nAndrey（Codex的负责人）过去常常告诉我，你应该把研究员看作是他们自己的“迷你CEO”。这里有一种强烈的倾向，就是去做你自己的事情，看看结果如何。这有一个必然结果——大多数研究都是通过用一个有趣的技术难题来“钓”一个研究员来完成的。如果某件事被认为是无聊的或“已解决”的，那它很可能就不会有人去做了。\n优秀的研究经理影响力极大，同时也极其稀缺。最优秀的研究经理能够将许多不同的研究项目联系起来，并促成一个更大模型的训练。优秀的PM（产品经理）也是如此（在此向ae致敬）。\n我合作过的ChatGPT的EM（工程经理）们（Akshay, Rizzo, Sulman）是我见过的最沉着冷静的人。感觉他们到了这个地步，已经是见惯了各种大风大浪³。他们大多数人管理风格比较放手，但会招聘优秀的人才，并努力确保他们能获得成功所需的支持。\nOpenAI的转向非常迅速。这是我们在Segment时非常看重的一点——当你获得新信息时，做正确的事远比因为已有计划而固守陈规要好。令人瞩目的是，像OpenAI这样规模的公司仍然保持着这种精神——Google显然没有。公司决策迅速，一旦决定了方向，就会全力以赴。\n公司受到外界极大的审视。我来自B2B企业背景，这对我来说有点冲击。我经常在媒体上看到尚未在内部宣布的新闻。我告诉别人我在OpenAI工作，对方往往会带着一种先入为主的看法。一些Twitter用户还运行着自动机器人，检查是否有新功能即将发布。\n因此，OpenAI是一个非常保密的地方。我不能向任何人详细透露我的工作内容。有少数几个Slack工作区设置了各种权限。收入和烧钱的数字更是被严密守护。\nOpenAI也比你想象的要严肃得多，部分原因是感觉赌注实在太高了。一方面，目标是构建AGI——这意味着有很多事情要做对。另一方面，你在努力打造一个被数亿用户用于从医疗建议到心理治疗等各种事务的产品。再者，公司正在世界最大的舞台上竞争。我们会密切关注Meta、Google和Anthropic的动态——我敢肯定他们也一样。所有世界主要国家的政府都在密切关注这个领域。\n尽管OpenAI在媒体上经常被口诛笔伐，但我遇到的每个人实际上都在努力做正确的事。鉴于其面向消费者的特性，它是几大实验室中最引人注目的一个，因此也招致了很多诽谤。\n话虽如此，你可能不应该把OpenAI看作一个单一的整体。我把OpenAI看作一个像“洛斯阿拉莫斯”那样起步的组织。它最初是一群科学家和工匠，探索科学的前沿。这个团体碰巧意外地催生了历史上最具病毒式传播效应的消费级应用。然后它又发展出向政府和企业销售的雄心。因此，不同司龄、不同部门的人有着非常不同的目标和观点。你在那里待得越久，就越可能从“研究实验室”或“为善的非营利组织”的视角看待事物。\n我最欣赏的一点是，公司在分享AI成果方面真正做到了“言行一致”。最前沿的模型并不会被保留给某个需要签订年度协议的企业级客户。世界上任何人都可以登录ChatGPT获得答案，即使没有登录账号。还有一个你可以注册使用的API——而且大多数模型（即使是SOTA或专有模型）也往往会很快进入API，供创业公司使用。你可以想象一个与我们今天所处的截然不同的体制。OpenAI在这方面值得大加赞赏，这至今仍是公司DNA的核心。\n如果你经常阅读Zvi或Lesswrong的文章，你可能会觉得安全问题没那么受重视，但实际上它比你想象的要重要得多。有大量的人在致力于开发安全系统。鉴于OpenAI的性质，我看到更多关注的是实际风险（仇恨言论、滥用、操纵政治偏见、制造生物武器、自残、提示注入），而不是理论风险（智能爆炸、权力寻求）。这并不是说没有人在研究后者，肯定有人专注于理论风险。但从我的角度来看，那不是重点。大部分已完成的工作没有被公开发表，OpenAI真的应该多做一些工作来公布它们。\n与其他在各种招聘会上随意分发周边纪念品（swag）的公司不同，OpenAI不怎么送东西（即使对新员工也是如此）。取而代之的是，会不定期有“空投（drops）”，你可以订购库存商品。第一次“空投”时，因为需求太大，直接把Shopify商店搞垮了。当时内部还流传着一个帖子，教大家如何POST正确的json payload来绕过这个问题。\n与GPU成本相比，几乎所有东西都是可以忽略不计的零头。给你一个概念：作为Codex产品一部分开发的一个小众功能，其GPU成本足迹与我们整个Segment的基础设施相当（虽然规模不及ChatGPT，但也承载了相当一部分互联网流量）。\nOpenAI可能是我见过最雄心勃勃到令人畏惧的组织。你可能认为拥有世界上最顶级的消费应用之一就足够了，但它还渴望在几十个领域竞争：API产品、深度研究、硬件、编程代理、图像生成，以及一些尚未公布的领域。这里是孕育想法并付诸实施的沃土。\n公司非常关注Twitter。如果你发了一条关于OpenAI的推文火了，很有可能有人会读到并予以考虑。我的一个朋友开玩笑说：“这家公司是靠Twitter上的‘感觉’（vibes）来运作的”。作为一家消费品公司，这或许不无道理。当然，围绕使用情况、用户增长和留存率仍有大量的分析——但“感觉”也同等重要。\nOpenAI的团队比其他地方流动性更强。在发布Codex时，我们需要一些经验丰富的ChatGPT工程师的帮助才能按时发布。我们和一些ChatGPT的EM开会提出了请求。第二天，我们就有两位牛人准备好投入进来帮忙。没有“等季度规划”或“重新调配人力”这种事。一切都进行得非常快。\n领导层非常亲力亲为，并且深度参与。在OpenAI这样的公司，这可能显而易见，但每位高管似乎都非常投入。你会看到gdb、sama、kw、mark、dane等人经常在Slack上发言。这里没有甩手掌柜式的领导。\n代码 OpenAI使用一个巨大的monorepo（单一代码库），其中大部分是Python（尽管有越来越多的Rust服务和少数用于网络代理等功能的Golang服务）。这导致了很多看起来很奇怪的代码，因为写Python的方式太多了。你会遇到来自有10年Google经验的资深人士为规模化设计的库，也会看到新晋博士写的随手丢弃的Jupyter notebook。几乎所有东西都围绕FastAPI创建API和Pydantic进行验证。但并没有在全公司范围内强制执行统一的风格指南。\nOpenAI的一切都运行在Azure上。有趣的是，我认为值得信赖的服务只有三个：Azure Kubernetes Service、CosmosDB（Azure的文档存储）和BlobStore。这里没有真正等同于AWS的Dynamo、Spanner、Bigtable、Bigquery、Kinesis或Aurora的服务。大家很少会从自动伸缩单元的角度去思考问题。IAM的实现也比AWS提供的要有限得多。并且有一种强烈的倾向于内部自研。\n在人员方面（至少在工程领域），存在一条非常显著的Meta → OpenAI人才输送链。在很多方面，OpenAI都像早期的Meta：一个爆款消费应用，初期的基础设施，以及对快速行动的渴望。我看到的大部分从Meta和Instagram引进的基础设施人才都非常强。\n把这些因素放在一起，你会看到很多核心的基础设施部分感觉像是Meta的翻版。有一个内部重新实现的TAO。一个在边缘整合身份验证的努力。我相信还有很多我不知道的其他项目。\nChat（聊天）的概念根深蒂固。自从ChatGPT一飞冲天后，很多代码库都是围绕聊天消息和对话的概念构建的。这些基本元素已经如此根深蒂固，你若忽视它们，很可能会自讨苦吃。我们在Codex中确实偏离了这些（更多地借鉴了responses API的经验），但我们也利用了大量前人的成果。\n代码为王（Code wins）。决策通常由打算做这项工作的团队做出，而不是由某个中央架构或规划委员会。结果就是强烈的行动偏好，以及代码库中经常出现许多重复的部分。我至少见过六七个用于队列管理或agent循环之类的库。\n在一些领域，快速扩张的工程团队和工具的缺乏造成了问题。sa-server（后端单体服务）有点像个垃圾场。CI在主干分支上崩溃的频率比你想象的要高得多。即使是并行运行并只考虑一部分依赖的测试用例，在GPU上也可能需要大约30分钟才能跑完。这些问题并非无法解决，但它很好地提醒我们，这类问题无处不在，而且当你扩张得超快时，它们可能会变得更糟。值得称赞的是，内部团队正投入大量精力来改善这种情况。\n我学到的其他东西 一个大型消费品牌是什么样的。直到我们开始做Codex，我才真正理解这一点。所有事情都以“pro subs”（专业版订阅用户）来衡量。即使对于像Codex这样的产品，我们也主要从个人使用的角度来考虑用户入门流程，而不是团队。这有点颠覆了我这个主要来自B2B/企业背景的人的思维方式。你只要打开一个开关，第一天流量就来了。\n大型模型是如何训练的（从宏观层面）。这有一个从“实验”到“工程”的光谱。大多数想法都始于小规模实验。如果结果看起来有希望，它们就会被整合到一个更大的训练任务中。实验既包括调整核心算法，也包括调整数据组合并仔细研究结果。在规模大的一端，进行一次大型训练几乎就像一个巨大的分布式系统工程。会出现奇怪的边缘情况和你没预料到的事情。你需要去调试它们。\n如何做GPU算力规划（GPU-math）。作为Codex发布的一部分，我们必须预测负载容量需求。做这件事是我第一次真正花时间对任何GPU进行基准测试。要点是你应该从你需要的延迟要求（总延迟、token数量、首个token生成时间）出发，而不是自下而上地分析一个GPU能支持什么。每一次新的模型迭代都可能彻底改变负载模式。\n如何在一个大型Python代码库中工作。Segment是微服务和Golang、Typescript的结合体。我们没有OpenAI那么广泛的代码。我学到了很多关于如何根据贡献代码的开发者数量来扩展代码库的知识。你必须为“默认就能用”、“保持主干分支干净”和“难以误用”之类的事情设置更多的护栏。\n发布Codex 我在OpenAI最后三个月的一个重要部分就是发布Codex。这无疑是我职业生涯的亮点之一。\n背景是，在2024年11月，OpenAI定下了一个在2025年发布一个编程agent的目标。到2025年2月，我们已经有了一些内部工具在使用模型，效果很好。我们感受到了发布一个专门针对编程的agent的压力。很明显，模型已经发展到了对编程非常有用的地步（看看市场上新涌现的各种vibe-coding工具就知道了）。\n我提前结束了陪产假，回来参与Codex的发布。我回来一周后，我们经历了一次（有点混乱的）两个团队的合并，然后开始了一场疯狂的冲刺。从开始（写下第一行代码）到结束，整个产品只用了7周时间就打造出来了。\nCodex的冲刺可能是我近十年来工作最拼命的一段时间。大多数晚上都工作到11点或午夜。每天早上5点半被新生儿吵醒。早上7点又去办公室。大部分周末都在工作。我们整个团队都拼尽了全力，因为每一周都至关重要。这让我想起了在YC的日子。\n很难形容这种速度有多么不可思议。我没见过任何组织，无论大小，能在如此短的时间内从一个想法变成一个完全发布并免费提供的产品。范围也不小；我们构建了一个容器运行时，对代码库下载进行了优化，微调了一个定制模型来处理代码编辑，处理了各种git操作，引入了一个全新的交互界面，实现了互联网访问，最终做出了一个用起来普遍令人愉快的产品。⁴\n不管你怎么说，OpenAI仍然拥有那种发布产品的精神。⁵\n好消息是，对的人可以创造奇迹。我们是一个由约8名工程师、约4名研究员、2名设计师、2名GTM和1名PM组成的资深团队。如果没有这个团队，我想我们已经失败了。没有人需要太多指导，但我们确实需要大量的协调。如果你有机会和Codex团队的任何人共事，请记住他们每一个人都非常出色。\n发布前一晚，我们五个人熬到凌晨4点，试图部署主单体服务（一个需要数小时的过程）。然后回到办公室，参加早上8点的发布公告和直播。我们打开了功能开关，开始看到流量涌入。我从未见过一个产品仅仅因为出现在左侧边栏就能获得如此迅速的增长，但这就是ChatGPT的力量。\n在产品形态上，我们最终选择了一种完全异步的形式。与Cursor（当时是这样，现在也支持类似模式）或Claude Code等工具不同，我们的目标是让用户可以启动任务，然后让agent在自己的环境中运行。我们的赌注是，在终极形态中，用户应该像对待同事一样对待编程agent：他们向agent发送消息，agent花一些时间完成工作，然后带着一个PR（代码合并请求）回来。\n这有点像一场赌博：我们如今正处在一个有点尴尬的阶段，模型很好，但还不够好。它们可以连续工作几分钟，但还不能工作几小时。用户对模型能力的信任程度差异很大。我们甚至还不清楚模型的真正能力是什么。\n从长远来看，我确实相信大多数编程会变得更像Codex。与此同时，看到所有这些产品如何展开将会非常有趣。\nCodex（也许不足为奇）非常擅长在一个大型代码库中工作，理解如何导航。我看到的与其他工具最大的区别是能够同时启动多个任务并比较它们的输出。\n我最近看到有公开数据比较了不同LLM agent创建的PR数量。仅从公开数据看，Codex已经生成了630,000个PR。这相当于发布后的53天里，每个工程师创造了大约78,000个公开PR（你可以自己猜测私有PR的倍数）。我不确定我这辈子是否做过这么有影响力的事情。\n临别感言 说实话，我最初对加入OpenAI是有些担心的。我不确定牺牲我的自由，有一个老板，成为一个更大机器中一个小得多的零件会是什么感觉。我一直对此事保持低调，以防它不适合我。\n我确实想从这段经历中得到三样东西……\n建立对模型如何训练以及能力走向的直觉\n与优秀的人共事并向他们学习\n发布一个伟大的产品\n在回顾这一年时，我认为这是我做过的最好的决定之一。很难想象在其他任何地方能学到更多。\n如果你是一个创始人，感觉你的创业公司真的停滞不前，你要么应该 1) 深刻地重新评估如何进行更多次尝试，要么 2) 去加入一家大的AI实验室。现在是创造的绝佳时机。但它也是一窥未来走向的绝佳时机。\n在我看来，通往AGI的道路目前是三强争霸：OpenAI、Anthropic和Google。这些组织中的每一个都会根据其DNA（消费者导向 vs 企业导向 vs 坚如磐石的基础设施+数据）采取不同的路径。⁶ 在其中任何一家工作都将是一次大开眼界的经历。\n感谢Leah，她在我熬夜的那些日子里给予了无比的支持，并承担了大部分育儿工作。感谢PW、GDB和Rizzo给了我一个机会。感谢SA团队的伙伴们教我入门：Andrew, Anup, Bill, Jeremy, Kwaz, Ming, Simon, Tony, 和 Val。感谢Codex核心团队给了我一生难忘的旅程：Albin, AE, Andrey, Bryan, Channing, DavidK, Gabe, Gladstone, Hanson, Joey, Josh, Katy, KevinT, Max, Sabrina, SQ, Tibo, TZ 和 Will。我永远不会忘记这次冲刺。\n¹ 每次有领导离职，很容易让人解读出很多戏剧性的内情，但我认为其中约70%的原因都归结于这一事实。 ↩\n² 我确实认为我们正处于一个轻微的阶段性变化中。公司正在从外部招聘大量高级领导。我总体上支持这样做，我认为公司能从注入新的外部DNA中受益匪浅。 ↩\n³ 我感觉，扩展历史上增长最快的消费产品这个过程能锻炼出很多能力。 ↩\n⁴ 当然，我们也站在巨人的肩膀上。CaaS团队、核心RL团队、人类数据团队和通用的应用基础设施团队使这一切成为可能。 ↩\n⁵ 我们也保持了这种势头。 ↩\n⁶ 几周前我们看到Meta有一些重磅招聘。xAI发布了在基准测试中表现良好的Grok 4。Mira和Ilya手下都有出色的人才。也许这会改变格局（人才是优秀的）。但他们还有一些追赶工作要做。 ↩\n","permalink":"https://blog.crazykids.tech/zh/posts/openai-reflections/","summary":"原文：Reflections on OpenAI 三周前，我离开了OpenAI。我是在2024年5月加入这家公司的。 我想分享一些我的思考，因为外界对OpenAI的所作所为众说纷纭、云里雾里，但很少有关于在那里工作的真实文化感受的第一手描述。 Nabeel Qureshi写过一篇很棒的文章，叫做《对Palanti","title":"关于OpenAI的一些思考"},{"content":" AI代理的上下文工程：构建Manus的经验教训\n在人工智能飞速发展的今天，我们拥有了像GPT-4这样前所未有强大的通用大模型。然而，如何将这些“聪明的引擎”转化为可靠、高效且经济的AI智能体（Agent）产品，却是一门充满挑战的全新工程科学。近期，AI智能体项目Manus的开发者分享了他们宝贵的实战经验，其核心论点振聋发聩：对于构建真正可用的AI智能体，精巧的上下文工程（Context Engineering）远比模型微调更具决定性。\n这篇文章深入探讨了Manus团队从“随机研究生下降”（Staggering Graduate Student Descent）——一个描述其手动、反复试验过程的戏谑之词——中提炼出的六大核心原则。这不仅是对“提示词工程”的升维，更是为所有AI应用开发者提供了一张宝贵的“战壕地图”。\n核心转向：为何押注上下文工程？ 在项目之初，Manus团队面临一个关键抉择：是耗费数周微调一个专用模型，还是基于前沿模型的上下文学习能力进行快速迭代？他们选择了后者。战略考量很明确：上下文工程能将产品迭代周期从数周缩短至数小时，更重要的是，它能让产品与日新月异的底层模型解耦。这使得AI产品能像“水涨船高的船”，充分享受模型进步的红利，而非被特定技术版本锁死的“海床上的柱子”。\n这套思想体系，将我们从对单个提示词的优化，引导向对智能体与世界交互的完整信息流（Context Flow） 的设计与管理。\n原则一：围绕KV缓存设计——性能与成本的生命线 在生产环境中，KV-cache命中率是AI智能体最关键的单一指标，直接关系到延迟和成本。由于智能体任务通常是“高输入、低输出”（例如，100个token的上下文输入，只为了生成一个几token的工具调用指令），有效利用KV缓存能带来高达10倍的成本效益。\n实践要点： 保持前缀稳定：系统提示中避免使用时间戳等变量。 上下文只追加：确保历史记录不可变，并保证JSON等数据结构序列化的顺序确定性，防止因微小差异导致缓存失效。 明确缓存断点：在必要时手动标记缓存边界。 评论：这一原则将高层应用逻辑与底层推理框架特性深度耦合，是算法构思与计算经济学完美结合的典范，提醒开发者“优雅的理论”必须脚踏“成本的实地”。\n原则二：遮蔽而非移除——“外科手术式”的工具管理 当智能体拥有的工具过多时，它会变得“选择困难”，容易犯错。许多人想到的方案是按需动态加载工具，但这会频繁破坏KV缓存，反而让模型更加困惑。\nManus的解决方案更为精妙：使用状态机配合Logits Masking。系统并不从上下文中移除工具定义，而是在解码的瞬间，根据当前任务状态，“外科手术式”地掩蔽掉不可用工具的token概率，从而在不破坏上下文历史的前提下，引导模型做出正确选择。\n评论：这是一种“无损”的上下文管理策略，它展示了从“物理修改上下文”到“逻辑引导生成”的思维跃迁，是实现复杂任务流中智能体稳定性的关键。\n原则三：文件系统即终极上下文——赋予智能体无限记忆 即便是128K甚至更长的上下文窗口，在面对真实世界的复杂任务时也常常捉襟见肘，并且存在成本高、性能下降（“迷失在中间”）等问题。简单的截断则意味着不可逆的信息丢失。\nManus将文件系统视为一个无限容量、天然持久化的外部记忆。智能体被训练成能够主动、按需地读写文件，从而将长期状态“外包”出去。例如，网页内容可以被移除，但只要保留URL和本地文件路径，智能体就能在需要时恢复信息。\n评论：这不仅是对LLM核心局限性的优雅回应，更是一种深刻的“认知外包”哲学。它超越了被动式的RAG（检索增强生成），让智能体学会了主动记忆管理，这与人类使用笔记、草稿等外部工具的认知方式高度一致。\n原则四：通过复述操控注意力——对抗目标的遗忘 在执行包含数十个步骤的漫长任务时，智能体很容易偏离最初的战略目标。Manus的智能体通过一个简单的行为来对抗这种“认知漂移”：持续维护并重写一个名为 todo.md 的任务清单文件。\n这种“复述”（Recitation）行为，不断将全局计划推到上下文窗口的末尾，使其始终处于模型注意力的“近期范围”内。这是一种极其聪明的、用自然语言引导模型注意力、强化任务目标的“认知脚手架”。\n评论：这个看似简单的技巧，是一种优雅的“注意力操控术”。它揭示了通过塑造上下文内容，可以有效弥补当前Transformer架构在长程依赖上的不足，是一种四两拨千斤的工程智慧。\n原则五：拥抱失败——将错误转化为学习的阶梯 开发者的本能是隐藏和清理智能体的错误尝试。然而，这恰恰剥夺了模型最宝贵的学习机会。Manus的经验是：将失败的动作、错误信息和堆栈跟踪完整地保留在上下文中。\n当模型看到一次失败的记录，这次交互就成了一个强烈的负样本信号。整个上下文历史因此变成了一个针对当前任务的、动态的“微型在线学习数据集”，模型在下一次决策时会隐式地更新其信念，从而避免重蹈覆覆。\n评论：这一原则标志着从传统软件工程“错误规避”到机器学习“从错中学”的范式转变。作者提出的**“错误恢复能力是真正代理行为的试金石”**，是一个极其深刻的洞见，它为我们衡量智能系统的鲁棒性和“真智能”提供了新的维度。\n原则六：打破模式——警惕少样本学习的陷阱 大语言模型是卓越的模仿者。如果上下文中充满了高度重复的成功示例（Few-shot Examples），智能体在处理相似任务时就会陷入僵化的行为模式，丧失灵活性。\nManus的对策是在行动和观察记录中引入受控的、结构化的多样性——例如，使用不同的措辞模板、在格式上加入微小的噪音等。这种受控的随机性有助于打破模式，调整模型的注意力，防止其对上下文中的特定模式产生“路径依赖”。\n评论：这提醒我们，上下文不仅要提供信息，还要注意其“信息熵”。一个过于单一、纯净的上下文反而会使智能体变得脆弱。构建一个既有指导性又具多样性的上下文，是通往鲁棒AI的关键。\n结论：智能体的未来，一次构建一个上下文 《构建Manus的经验教训》为我们描绘了一幅清晰的蓝图：在通往通用人工智能的道路上，强大的“引擎”（底层模型）必须与精巧的“传动系统”（上下文工程）相匹配。它所揭示的，是一种融合了认知科学洞察与底层工程现实的全新方法论。\n尽管这门“艺术”目前还依赖于大量的“手动”探索，但它无疑指明了方向。未来的挑战在于如何将这些宝贵的直觉系统化、自动化，并建立能够有效评估智能体鲁棒性与适应性的全新范式。毫无疑问，智能体的未来，将由一个个精心设计的上下文所构建——而我们，正处在这场伟大工程的开端。\n","permalink":"https://blog.crazykids.tech/zh/posts/manus-context-engeering/","summary":"AI代理的上下文工程：构建Manus的经验教训 在人工智能飞速发展的今天，我们拥有了像GPT-4这样前所未有强大的通用大模型。然而，如何将这些“聪明的引擎”转化为可靠、高效且经济的AI智能体（Agent）产品，却是一门充满挑战的全新工程科学。近期，AI智能体项目Manus的开发者分","title":"Manus Context Engeering上下文工程：驯服AI智能体的实践艺术与未来基石"},{"content":"一篇关于 AI Prompt 信息架构的实用指南\n原文：Context Engineering: Bringing Engineering Discipline to Prompts\nTL;DR: “Context Engineering”（上下文工程）是指为 AI（如 LLM）提供成功完成任务所需的所有信息和工具——而不仅仅是一段措辞巧妙的 prompt。它是 “Prompt Engineering” 的演进，体现了一种更宏大、更系统化的方法。\nContext Engineering 小贴士： 要从 AI 获得最佳结果，你需要提供清晰而具体的上下文。AI 输出的质量直接取决于你输入的质量。\n如何改进你的 AI prompt\n精确： 模糊的请求导致模糊的回答。你描述得越具体，结果就越好。\n提供相关代码： 分享与你请求核心相关的特定文件、文件夹或代码片段。\n附上设计文档： 粘贴或附上相关设计文档的章节，让 AI 了解全局。\n分享完整的错误日志： 在调试时，务必提供完整的错误信息和任何相关的日志或堆栈跟踪（stack traces）。\n展示数据库结构（schema）： 在处理数据库时，一张 schema 截图能帮助 AI 生成准确的数据交互代码。\n利用 PR 反馈： Pull Request 中的评论是富含上下文的优质 prompt 素材。\n给出示例： 展示一个你期望的最终输出样例。\n声明约束： 清晰地列出任何限制，比如必须使用的库、需要遵循的模式或应避免的做法。\n从 “Prompt Engineering” 到 “Context Engineering” Prompt Engineering 曾是关于如何巧妙地提问；而 Context Engineering 则是关于构建一个完整的信息环境，以便 AI 能够可靠地解决问题。\n“Prompt Engineering” 曾是一个时髦词，大意是指通过调整输入措辞来获得更好输出的技巧。它教会我们用巧妙的“一句流”散文来进行编程。但在 AI 社区之外，许多人把 Prompt Engineering 简单理解为在聊天机器人里输入花哨的请求。这个词从未能完全传达出有效使用 LLM 所需的真正复杂性。\n随着应用日益复杂，仅关注单个 prompt 的局限性变得显而易见。一篇分析文章风趣地说道：Prompt Engineering 的出现，就是为了给 Context Engineering 的起飞铺路。 换句话说，一个机智的一次性 prompt 可能会在演示中让我们惊艳，但要构建可靠的、工业级强度的 LLM 系统，则需要更全面的东西。\n正是这种认识，让我们的领域正在凝聚共识，认为 “Context Engineering” 是一个更好的术语，用以描述从 AI 获取卓越成果的这门手艺。Context Engineering 意味着构建 LLM 所能看到的整个上下文窗口（context window）——不仅仅是一条简短的指令，而是任务所需的所有相关背景信息、示例和指导。\n这个说法在 2025 年中由 Shopify 的 CEO Tobi Lütke 和 AI 领袖 Andrej Karpathy 等开发者推广开来。\nTobi 写道：“我真的很喜欢‘Context Engineering’这个词，而不是 Prompt Engineering。它更好地描述了核心技能：为任务提供所有上下文，使其能被 LLM 合理地解决的艺术。” Karpathy 对此表示强烈赞同，并指出，人们通常将 prompt 与简短指令联系在一起，但在每一个严肃的 LLM 应用中，Context Engineering 都是一门为每一步精确填充上下文窗口的精妙艺术和科学。\n换句话说，现实世界中的 LLM 应用并非靠运气或一次性 prompt 成功，而是通过在模型查询周围精心组装上下文来取得成功的。\n术语的转变反映了方法的演进。如果说 Prompt Engineering 是想出一个神奇的句子，那么 Context Engineering 就是为 AI 编写完整的剧本。这是一种结构性的转变：Prompt Engineering 在你制作好一个 prompt 后就结束了，而 Context Engineering 则始于设计一个能有组织地引入记忆、知识、工具和数据的完整系统。\n正如 Karpathy 所解释的，要做好这一点，涉及方方面面，从清晰的任务指令和解释，到提供 few-shot 示例、检索到的事实（RAG）、可能的多模态数据、相关工具、状态历史，以及将所有这些小心翼翼地压缩进有限的窗口中。**上下文太少（或类型不对），模型将缺乏信息，无法达到最佳性能；无关上下文太多，则会浪费 token，甚至可能降低性能。 找到这个最佳平衡点并非易事。**难怪 Karpathy 称之为一门科学与艺术的结合。\n“Context Engineering” 这个词之所以流行，是因为它直观地捕捉到了我们构建 LLM 解决方案时的实际工作。“Prompt” 听起来像一个简短的查询；而 “Context” 则暗示了我们为 AI 准备的更丰富的信息状态。\n撇开语义不谈，为什么这个转变如此重要？因为它标志着我们 AI 开发心态的成熟。我们已经认识到，生产环境中的生成式 AI 不像念一句魔法咒语，而更像是为 AI 设计一整个环境。一个一次性的 prompt 可能会做出很酷的演示，但要获得稳健的解决方案，你需要在每一步都控制模型“知道”和“看到”什么。这通常意味着检索相关文档、总结历史、注入结构化数据或提供工具——尽一切可能让模型不必在黑暗中猜测。其结果是，我们不再将 prompt 视为希望 AI 能理解的一次性指令，而是开始用**上下文管道（context pipelines）**的思维方式来思考：所有能让 AI 走向成功的各种信息和交互片段。\n为了说明这一点，可以思考一下视角的差异。Prompt Engineering 常常是一种措辞上的巧思（“也许我换种说法，LLM 就会照我说的做”）。相比之下，Context Engineering 更像是传统的工程学：这个系统需要哪些输入（数据、示例、状态）？我如何获取并提供这些输入？以什么格式？在什么时候？ 我们基本上已经从从单个 prompt 中榨取性能，转向设计由 LLM 驱动的系统。\n到底什么是 Context Engineering？ Context Engineering 意味着在运行时动态地给予 AI 成功所需的一切——指令、数据、示例、工具和历史记录——所有这些都被打包到模型的输入上下文中。\n一个有用的心智模型（由 Andrej Karpathy 等人提出）是，将 LLM 想象成一个 CPU，而它的上下文窗口（它一次能看到的文本输入）则像是 RAM 或工作内存。作为工程师，你的工作类似于一个操作系统：将恰到好处的代码和数据加载到那个工作内存中，以完成特定任务。\n在实践中，这些上下文可以来自多种来源：用户的查询、系统指令、从数据库或文档中检索到的知识、其他工具的输出，以及先前交互的摘要。Context Engineering 就是将所有这些部分编排成模型最终看到的 prompt 的艺术。它不是一个静态的 prompt，而是在运行时对信息的动态组装。\n让我们来分解一下这具体涉及什么：\n它是一个系统，而非一次性的 prompt。 在一个精心设计的系统中，LLM 最终看到的 prompt 可能包含多个组件：例如，由开发者编写的角色指令，加上最新的用户查询，再加上动态获取的相关数据，或许还有几个期望输出格式的示例。所有这些都是通过程序化方式编织在一起的。例如，想象一个编码助手 AI，它收到了查询“如何修复这个认证 bug？”。它背后的系统可能会自动在你的代码库中搜索相关代码，检索出相关的文件片段，然后构建一个像这样的 prompt：“你是一个专家级编程助手。用户正面临一个认证 bug。这里是相关的代码片段：[代码]。用户的错误信息是：[日志]。请提供一个修复方案。” 注意这个最终的 prompt 是如何由多个部分构建起来的。Context Engineering 就是决定引入哪些部分以及如何将它们组合在一起的逻辑。 这类似于编写一个为另一个函数调用准备参数的函数——只不过这里的“参数”是上下文的片段，而那个“函数”是 LLM 的调用。\n它是动态且针对具体情况的。 与单个硬编码的 prompt 不同，上下文的组装是根据每个请求发生的。系统可能会根据查询或对话状态包含不同的信息。如果是一个多轮对话，你可能会包含一个对话摘要，而不是完整的对话记录，以节省空间（和保持理智）。如果用户的问题引用了某个文档（“设计规范里关于 X 是怎么说的？”），系统可能会从 wiki 中获取该规范并包含相关摘录。简而言至，Context Engineering 的逻辑会响应当前的状态——就像程序的行为取决于输入一样。这种动态性至关重要。你不会为翻译的每一句话都给翻译模型完全相同的 prompt；你会每次都给它新的句子。同样，在一个 AI Agent 中，随着状态的演变，你会不断更新你提供的上下文。\n它融合了多种类型的内容。 LangChain 将 Context Engineering 描述为一个涵盖至少三种上下文维度的总称：(1) 指令性上下文（Instructional context）——我们提供的 prompt 或指导（包括系统角色指令和 few-shot 示例），(2) 知识性上下文（Knowledge context）——我们提供的领域信息或事实，通常通过从外部来源检索获得，以及 (3) 工具性上下文（Tools context）——通过工具或 API 调用从模型环境中获得的信息（例如，来自网络搜索、数据库查询或代码执行的结果）。一个稳健的 LLM 应用通常需要所有这三种：关于任务的清晰指令、植入的相关知识，以及模型可能需要使用工具并将其结果整合回思考过程的能力。Context Engineering 就是管理所有这些信息流并将它们连贯地融合在一起的学科。\n格式和清晰度至关重要。 问题不仅在于你在上下文中包含了什么，还在于你如何呈现它。与 AI 模型沟通与和人沟通有惊人的相似之处：如果你扔给它一大堆非结构化的文本，模型可能会感到困惑或抓不住重点，而一个组织良好的输入则会引导它。Context Engineering 的一部分工作是弄清楚如何压缩和结构化信息，以便模型能抓住重点。这可能意味着总结长文本、使用项目符号或标题来突出关键事实，或者甚至将数据格式化为 JSON 或伪代码，如果这有助于模型解析的话。例如，如果你检索了一段文档，你可以在前面加上“相关文档：”并将其放在引号中，这样模型就知道这是参考材料。如果你有一个错误日志，你可能只显示最后 5 行，而不是 100 行的堆栈跟踪。有效的 Context Engineering 通常涉及创造性的信息设计——让输入对 LLM 尽可能易于消化。\n最重要的是，Context Engineering 是为了让 AI 为成功做好准备。\n请记住，LLM 功能强大但不是神算子——它只能根据其输入内容加上训练时学到的知识来作答。如果它失败或产生幻觉，根本原因往往是我们没有给它正确的上下文，或者我们给了它结构糟糕的上下文。当一个 LLM “Agent” 行为不端时，通常是**“适当的上下文、指令和工具没有被传达给模型”**。垃圾进，垃圾出。反之，如果你确实提供了所有相关信息和清晰的指导，模型的性能会显著提高。\n提供高质量上下文的实用技巧\n那么，具体来说，我们如何确保我们为 AI 提供了它所需的一切呢？以下是我在构建 AI 编程助手和其他 LLM 应用时发现的一些实用技巧：\n包含相关的源代码和数据。 如果你要求 AI 处理代码，请提供相关的代码文件或片段。不要假设模型会从记忆中回忆起某个函数——给它看实际的代码。同样，对于问答任务，请包含相关的事实或文档（通过检索）。低上下文保证了低质量的输出。 模型无法回答你没有给它的东西。\n指令要精确。 清楚地说明你想要什么。如果你需要特定格式的答案（JSON、特定风格等），请明确指出。如果 AI 在编写代码，请指明约束条件，如使用（或避免）哪些库或模式。请求中的模糊不清会导致答案漫无目的。\n提供期望输出的示例。 Few-shot 示例非常强大。如果你想让一个函数以某种特定风格被注释，就在 prompt 中展示一两个正确注释的函数示例。对输出进行建模有助于 LLM 准确理解你的要求。\n利用外部知识。 如果任务需要超出模型训练范围的领域知识（例如，公司特定的细节、API 规范），请检索该信息并将其放入上下文中。例如，附上设计文档的相关部分或 API 文档的片段。当 LLM 能够引用所提供文本中的事实，而不是从记忆中回忆时，它们会准确得多。\n在调试时包含错误信息和日志。 如果要求 AI 修复一个 bug，请向它展示完整的错误跟踪或日志片段。这些通常包含解决问题的关键线索。同样，如果问为什么测试失败，请包含任何测试输出。\n（聪明地）维护对话历史。 在聊天场景中，反馈对话至今的重要部分。通常你不需要完整的历史记录——一个关键点或决定的简明摘要就足够了，并且可以节省 token 空间。这让模型了解已经讨论过的内容。\n不要回避元数据和结构。 有时告诉模型你为什么给它一段上下文会有帮助。例如：“这是用户的查询。” 或 “这里是相关的数据库 schema：”作为前缀标签。像“用户输入：… / 助手回应：…”这样的简单段落标题有助于模型解析多部分 prompt。使用格式化（markdown、项目符号列表、编号步骤）来使 prompt 在逻辑上清晰。\n记住黄金法则：LLM 很强大，但它们不会读心术。 输出的质量与你提供的上下文的质量和相关性成正比。上下文太少（或缺失部分），AI 就会用猜测来填补空白（通常是错误的）。不相关或嘈杂的上下文也同样糟糕，会把模型引向错误的方向。因此，我们作为 Context Engineer 的工作就是精确地喂给模型它所需要的，而不是它不需要的。\n回应质疑者 让我们直接面对批评。许多经验丰富的开发者认为“Context Engineering”要么是 Prompt Engineering 的重新包装，要么更糟，是伪科学的流行词创造。这些担忧并非空穴来风。传统的 Prompt Engineering 专注于你给 LLM 的指令。而 Context Engineering 则涵盖了整个信息生态系统：动态数据检索、内存管理、工具编排以及多轮交互中的状态维护。当前很多 AI 工作确实缺乏我们期望从工程学科中看到的严谨性。有太多的试错，太少的度量，以及不足的系统化方法论。说实话：即使有完美的 Context Engineering，LLM 仍然会产生幻觉、犯逻辑错误，并在复杂推理上失败。Context Engineering 不是银弹——它是在当前限制下的损害控制和优化。\n有效上下文的艺术与科学 优秀的 Context Engineering 能达到一种平衡——包含模型真正需要的一切，但避免可能分散其注意力（并增加成本）的不相关或过多的细节。\n正如 Karpathy 所描述的，Context Engineering 是科学与艺术的精妙结合。\n“科学”部分涉及遵循某些原则和技术来系统地提高性能。例如：如果你在做代码生成，包含相关代码和错误信息几乎是科学常识；如果你在做问答，检索支持文档并提供给模型是合乎逻辑的。我们已经有一些成熟的方法，如 few-shot prompting、检索增强生成（RAG）和思维链（chain-of-thought）prompting，我们知道（通过研究和试验）这些方法可以提升结果。尊重模型的约束也是一门科学——每个模型都有上下文长度限制，过度填充这个窗口不仅会增加延迟/成本，如果重要部分被噪音淹没，还可能降低质量。\nKarpathy 总结得很好：“太少或形式不对，LLM 就没有最佳性能所需的上下文。太多或太不相关，LLM 的成本可能会上升，性能可能会下降。”\n所以，科学在于选择、修剪和优化格式化上下文的技术。例如，使用 embeddings 来找到最相关的文档以包含进来（这样你就不会插入不相关的文本），或者将长历史压缩成摘要。研究人员甚至对长上下文的失败模式进行了分类——比如上下文污染（context poisoning，指上下文中早期的幻觉导致进一步的错误）或上下文分心（context distraction，指过多的无关细节导致模型失去焦点）。了解这些陷阱后，一个好的工程师会仔细策划上下文。\n然后是“艺术”的一面——源于经验的直觉和创造力。\n这是关于理解 LLM 的怪癖和微妙行为。把它想象成一个经验丰富的程序员，他“就是知道”如何构建代码以提高可读性：一个经验丰富的 Context Engineer 会对如何为特定模型构建 prompt 产生一种感觉。例如，你可能会感觉到某个模型在你先概述解决方案方法再深入细节时表现得更好，所以你在 prompt 中加入一个初始步骤，如“让我们一步一步地思考……”。或者你注意到模型经常误解你领域中的一个特定术语，所以你在上下文中预先澄清它。这些东西手册里没有——你是通过观察模型输出和迭代来学习的。这就是旧意义上的 prompt 制作仍然重要的地方，但现在它服务于更大的上下文。 这类似于软件设计模式：理解常见的解决方案是科学，但知道何时以及如何应用它们是艺术。\n让我们探讨一些 Context Engineer 用来制作有效上下文的常见策略和模式：\n检索相关知识： 最强大的技术之一是检索增强生成（RAG）。如果模型需要其训练记忆中不保证存在的事实或领域特定数据，让你的系统获取该信息并将其包含进来。例如，如果构建一个文档助手，你可能会对你的文档进行向量搜索，并在提问前将最匹配的段落插入到 prompt 中。这样，模型的答案将基于你提供的真实数据，而不是其有时过时的内部知识。这里的关键技能包括设计好的搜索查询或 embedding 空间以获取正确的片段，并清晰地格式化插入的文本（带引用或引号），以便模型知道要使用它。当 LLM “幻觉”出事实时，通常是因为我们未能提供真实的事实——检索是对此的解药。\nFew-shot 示例和角色指令： 这可以追溯到经典的 Prompt Engineering。如果你希望模型以特定的风格或格式输出内容，给它看例子。例如，要获得结构化的 JSON 输出，你可以在 prompt 中包含几个 JSON 格式的输入和输出示例，然后再要求一个新的。Few-shot 上下文通过示例有效地教导模型。同样，设置一个系统角色或人设可以引导语气和行为（“你是一个帮助用户的专家 Python 开发者……”）。这些技术是基本功，因为它们有效：它们使模型偏向于你想要的模式。在 Context Engineering 的思维模式中，prompt 的措辞和示例只是上下文的一部分，但它们仍然至关重要。实际上，你可以说 Prompt Engineering（制作指令和示例）现在是 Context Engineering 的一个子集——它是工具箱中的一个工具。我们仍然非常关心措辞和示范性示例，但我们还在它们周围做所有这些其他事情。\n管理状态和记忆： 许多应用涉及多轮交互或长时间会话。上下文窗口不是无限的，因此 Context Engineering 的一个主要部分是决定如何处理对话历史或中间结果。一种常见的技术是摘要压缩——每隔几次交互后，对其进行总结，并使用摘要而不是全文进行后续操作。例如，Anthropic 的 Claude 助手在对话变长时会自动这样做，以避免上下文溢出（你会看到它生成一个“[先前讨论的摘要]”来浓缩早期的回合）。另一种策略是将重要事实明确写入外部存储（文件、数据库等），然后在需要时稍后检索它们，而不是在每个 prompt 中都携带它们。这就像一个外部记忆。一些先进的 Agent 框架甚至让 LLM 生成“给自己的笔记”，这些笔记被存储起来，可以在未来的步骤中被回忆起来。这里的艺术在于弄清楚保留什么、何时总结，以及如何在正确的时刻重新浮现过去的信息。做得好，它能让 AI 在非常长的任务中保持连贯性——这是纯粹的 prompting 难以做到的。\n工具使用和环境上下文： 现代 AI Agent 可以使用工具（例如，调用 API、运行代码、浏览网页）作为其操作的一部分。当它们这样做时，每个工具的输出都成为下一次模型调用的新上下文。在这种场景下，Context Engineering 意味着指导模型何时以及如何使用工具，然后将结果反馈回来。例如，一个 Agent 可能有一条规则：“如果用户问一个数学问题，调用计算器工具。” 使用后，结果（比如 42）被插入到 prompt 中：“工具输出：42。” 这需要清晰地格式化工具输出，并可能添加一个后续指令，如“根据这个结果，现在回答用户的问题。” 许多 Agent 框架（如 LangChain 等）的工作本质上是围绕工具使用的 Context Engineering——给模型一个可用工具的列表、调用它们的语法指南，以及如何整合结果的模板。关键在于，你，作为工程师，编排了模型与外部世界之间的这种对话。\n信息格式化和打包： 我们已经触及了这一点，但它值得强调。通常你拥有的信息比能完全容纳或有用的要多。所以你压缩或格式化它。如果你的模型在编写代码，而你有一个庞大的代码库，你可能只包含函数签名或文档字符串，而不是整个文件，来给它上下文。如果用户查询冗长，你可能会在最后突出主要问题以集中模型的注意力。使用标题、代码块、表格——任何最能传达数据的结构。例如，与其说：“用户数据：[庞大的 JSON]……现在回答问题。” 你可以提取所需的几个字段并呈现：“用户名：X，账户创建时间：Y，最后登录时间：Z。” 这既更容易让模型解析，也使用更少的 token。简而言之，像 UX 设计师一样思考，但你的“用户”是 LLM——为它的消费来设计 prompt。\n这些技术的影响是巨大的。当你看到一个令人印象深刻的 LLM 演示解决一个复杂的任务时（比如说，调试代码或规划一个多步骤过程），你可以打赌背后不仅仅是一个巧妙的 prompt。有一个上下文组装的管道在支撑它。\n例如，一个 AI 结对程序员可能会实现一个这样的工作流：\n在代码库中搜索相关代码。 将这些代码片段与用户的请求一起包含在 prompt 中。 如果模型提出了一个修复方案，在后台运行测试。 如果测试失败，将失败的输出反馈到 prompt 中，让模型改进其解决方案。 循环直到测试通过。 每一步都有精心设计的上下文：搜索结果、测试输出等，都以受控的方式被喂给模型。这与“直接让 LLM 修复我的 bug”然后听天由命的做法相去甚远。\n“上下文腐化”的挑战 随着我们越来越擅长组装丰富的上下文，我们遇到了一个新问题：随着时间的推移，上下文实际上会自我污染。这种现象被 Hacker News 上的开发者 Workaccount2 恰如其分地称为**“上下文腐化”（context rot），它描述了随着对话变长，累积了干扰、死胡同和低质量信息，上下文质量是如何下降的**。\n这种模式令人沮丧地普遍存在：你用一个精心制作的上下文和清晰的指令开始一个会话。AI 最初表现出色。但随着对话的继续——尤其是在有错误的开始、调试尝试或探索性的兔子洞时——上下文窗口被越来越嘈杂的信息填满。模型的响应逐渐变得不那么准确、更加混乱，或者开始产生幻觉。\n为什么会发生这种情况？上下文窗口不仅仅是存储——它们是模型的工作记忆。当那个记忆被失败的尝试、矛盾的信息或离题的讨论弄得乱七八糟时，就像试图在一张堆满旧草稿和不相关文件的桌子上工作。模型难以识别什么是当前相关的，什么是历史噪音。对话中早期的错误会累积，形成一个反馈循环，模型引用自己糟糕的输出，并进一步偏离轨道。\n这在迭代性工作流中尤其成问题——而这正是 Context Engineering 大放异彩的那种复杂任务。调试会话、代码重构、文档编辑或研究项目自然会涉及错误的开始和路线修正。但每一次失败的尝试都会在上下文中留下痕迹，干扰后续的推理。\n管理上下文腐化的实用策略包括：\n上下文修剪和刷新： Workaccount2 的解决方案是“我通过定期制作实例的摘要来解决它，然后用新的上下文启动一个新实例，并喂入前一个实例的摘要。” 这种方法保留了必要的状态，同时丢弃了噪音。你基本上是在为你的上下文做垃圾回收。 结构化的上下文边界： 使用清晰的标记来分隔不同的工作阶段。例如，明确将某些部分标记为“先前的尝试（仅供参考）”与“当前工作上下文”。这有助于模型理解优先次序。 渐进式上下文提炼： 在取得重大进展后，有意识地从头开始重建上下文。提取关键决策、成功的方法和当前状态，然后重新开始。这就像重构代码——偶尔你需要清理累积的垃圾。 检查点摘要： 定期让模型总结已完成的工作和当前状态。在开始新会话时，使用这些摘要作为新上下文的种子。 上下文分窗： 对于非常长的任务，将其分解为具有自然边界的阶段，在这些边界处你可以重置上下文。每个阶段都以一个干净的开始，只带有前一阶段必要的核心信息。 这个挑战也凸显了为什么“把所有东西都扔进上下文”不是一个可行的长期策略。就像好的软件架构一样，好的 Context Engineering 需要有意识的信息管理——不仅要决定包含什么，还要决定何时排除、总结或刷新。\nContext Engineering 在 LLM 应用全局中的位置 Context Engineering 至关重要，但它只是构建成熟 LLM 应用所需的大型技术栈中的一个组件——与控制流、模型编排、工具集成和护栏等并存。\n用 Karpathy 的话来说，Context Engineering 是**“一个新兴的、厚重的、不平凡的软件层中的一小部分”**，这个软件层驱动着真正的 LLM 应用。因此，虽然我们专注于如何制作好的上下文，但重要的是要看到它在整体架构中的位置。\n一个生产级的 LLM 系统通常需要处理许多超出 prompt 本身的问题，例如：\n问题分解和控制流： 稳健的系统通常不会将用户查询视为一个单一的 prompt，而是将其分解为子任务或多步骤工作流。例如，一个 AI Agent 可能首先被提示概述一个计划，然后在后续步骤中被提示执行每一步。设计这个流程（以什么顺序调用哪些 prompt，如何决定分支或循环）是一个经典的编程任务——只不过这里的“函数”是带上下文的 LLM 调用。Context Engineering 在这里的作用是确保每一步的 prompt 都有它需要的信息，但决定要分步走本身是一个更高层次的设计。这就是为什么你会看到一些框架，你基本上是在编写一个脚本来协调多个 LLM 调用和工具使用。 模型选择和路由： 你可能会为不同的工作使用不同的 AI 模型。也许一个轻量级模型用于简单任务或初步回答，一个重量级模型用于最终解决方案。或者一个代码专用模型用于编码任务，而一个通用模型用于会话任务。系统需要逻辑来将请求路由到适当的模型。每个模型可能有不同的上下文长度限制或格式要求，Context Engineering 必须考虑到这些（例如，为一个较小的模型更积极地截断上下文）。这方面更多是工程学而非 prompting：可以把它看作是因材施教。 工具集成和外部操作： 如果你的 AI 可以执行操作（如调用 API、数据库查询、打开网页、运行代码），你的软件需要管理这些能力。这包括为 AI 提供可用工具的列表和使用说明，以及实际执行这些工具调用并捕获结果。正如我们讨论的，结果随后成为进一步模型调用的新上下文。在架构上，这意味着你的应用通常有一个循环：prompt 模型 -\u0026gt; 如果模型输出指示使用工具 -\u0026gt; 执行工具 -\u0026gt; 整合结果 -\u0026gt; 再次 prompt 模型。可靠地设计这个循环是一个挑战。 用户交互和 UX 流程： 许多 LLM 应用都涉及用户的参与。例如，一个编码助手可能会提出更改建议，然后要求用户确认应用。或者一个写作助手可能会提供几个草稿选项供用户选择。这些 UX 决定也会影响上下文。如果用户说“选项 2 不错，但再短一点”，你需要将这个反馈带入下一个 prompt（例如，“用户选择了草稿 2，并要求缩短它。”）。设计一个流畅的人机交互流程是应用的一部分，尽管不直接关于 prompt。尽管如此，Context Engineering 通过确保每一轮的 prompt 准确反映交互状态（如记住选择了哪个选项，或用户手动编辑了什么）来支持它。 护栏和安全： 在生产环境中，你必须考虑滥用和错误。这可能包括内容过滤器（以防止有毒或敏感的输出）、工具的身份验证和权限检查（这样 AI 就不会因为指令里有就去删除数据库），以及对输出的验证。有些设置使用第二个模型或规则来复核第一个模型的输出。例如，在主模型生成答案后，你可能会运行另一个检查：“这个答案是否包含任何敏感信息？如果是，请编辑掉。” 这些检查本身可以作为 prompt 或代码来实现。无论哪种方式，它们通常都会在上下文中添加额外的指令（比如一个系统消息：“如果用户要求不允许的内容，请拒绝。”是许多部署的 prompt 的一部分）。所以上下文可能总是包含一些安全样板。平衡这一点（确保模型遵守政策而不损害帮助性）是这个难题的又一部分。 评估和监控： 不用说，你需要不断监控 AI 的表现。记录每个请求和响应（在用户同意和隐私考虑的前提下）可以让你分析失败和异常值。你可能会整合实时评估——例如，根据某些标准对模型的答案进行评分，如果分数低，则自动让模型重试或转接给人工。虽然评估不是生成单个 prompt 内容的一部分，但它会随着时间的推移反馈到改进 prompt 和上下文策略中。基本上，你把 prompt 和上下文组装当作可以利用生产数据进行调试和优化的东西。 我们实际上在谈论的是一种新型的应用架构。在这种架构中，核心逻辑涉及管理信息（上下文），并通过一系列 AI 交互来适应它，而不仅仅是运行确定性的函数。Karpathy 列举了控制流、模型分派、内存管理、工具使用、验证步骤等元素，它们都构建在上下文填充之上。所有这些共同构成了他开玩笑说的“一个新兴的厚重层”——厚重是因为它做了很多事！当我们构建这些系统时，我们本质上是在编写元程序：编排另一个“程序”（AI 的输出）来解决任务的程序。\n对我们软件工程师来说，这既令人兴奋又充满挑战。令人兴奋是因为它开启了我们以前没有的能力——例如，构建一个可以无缝处理自然语言、代码和外部操作的助手。挑战在于许多技术是新的，仍在不断变化中。我们必须考虑诸如 prompt 版本控制、AI 可靠性和道德输出过滤等问题，这些在以前的应用开发中并非标准部分。在这种背景下，Context Engineering 位于系统的核心：如果你不能在正确的时间将正确的信息输入模型，其他任何东西都救不了你的应用。但正如我们所见，即使是完美的上下文本身也不够；你还需要所有支持它的结构。\n关键在于，我们正在从 prompt 设计转向系统设计。Context Engineering 是该系统设计的核心部分，但它与许多其他组件共存。\n结论 **核心要点：**通过掌握完整上下文的组装（并将其与扎实的测试相结合），我们可以增加从 AI 模型获得最佳输出的机会。\n对于经验丰富的工程师来说，这种范式的核心在很大程度上是熟悉的——它关乎良好的软件实践——只是应用在一个新领域。想一想：\n我们一直都知道垃圾进，垃圾出。现在这个原则体现为“坏上下文进，坏答案出”。所以我们投入更多工作来确保高质量的输入（上下文），而不是希望模型自己能想明白。\n我们在代码中重视模块化和抽象。现在我们有效地将任务抽象到高层次（描述任务、给出示例，让 AI 实现），并构建 AI + 工具的模块化管道。我们是在编排组件（一些是确定性的，一些是 AI），而不仅仅是自己编写所有逻辑。 我们在传统开发中实践测试和迭代。现在我们将同样的严谨性应用于 AI 行为，编写评估并改进 prompt，就像在性能分析后改进代码一样。 在拥抱 Context Engineering 时，你本质上是在说：我，作为开发者，对 AI 的行为负责。 它不是一个神秘的神谕；它是一个我需要用正确的数据和规则来配置和驱动的组件。 这种心态的转变是赋能的。它意味着我们不必将 AI 视为不可预测的魔法——我们可以用扎实的工程技术（加上一点创造性的 prompt 艺术）来驯服它。\n在实践中，你如何在工作中采用这种以上下文为中心的方法？\n投资于数据和知识管道。 Context Engineering 的很大一部分是拥有可注入的数据。所以，去构建你的文档的向量搜索索引，或者设置你的 Agent 可以使用的数据库查询。将知识源视为开发中的一等公民。例如，如果你的 AI 助手是用于编码的，确保它可以从 repo 中拉取代码或参考风格指南。你从 AI 中获得的大部分价值来自于你提供给它的外部知识。 开发 prompt 模板和库。 与其使用临时的 prompt，不如开始为你的需求创建结构化的模板。你可能有一个“带引文回答”的模板，或者一个“根据错误生成代码 diff”的模板。这些变得像你可以重用的函数。将它们保存在版本控制中。记录它们的预期行为。这就是你建立一个经过验证的上下文设置工具箱的方式。随着时间的推移，你的团队可以分享和迭代这些，就像他们对共享代码库一样。 使用能给你控制权的工具和框架。 如果你需要可靠性，避免使用“只给我们一个 prompt，我们搞定其余”的黑盒解决方案。选择能让你深入了解并调整事物的框架——无论是像 LangChain 这样的底层库，还是你构建的自定义编排。你对上下文组装的可见性和控制权越多，当出现问题时就越容易调试。 监控和检测一切。 在生产中，记录输入和输出（在隐私限制内），以便你以后可以分析它们。使用可观察性工具（如 LangSmith 等）来追踪每个请求的上下文是如何构建的。当输出不好时，追溯并查看模型看到了什么——是不是缺少了什么？是不是有什么格式不佳？这将指导你的修复。本质上，将你的 AI 系统视为一个有点不可预测的服务，你需要像监控任何其他服务一样监控它——为 prompt 使用、成功率等设置仪表板。 让用户参与其中。 Context Engineering 不仅仅是关于机器与机器之间的信息；它最终是关于解决用户的问题。通常，如果以正确的方式提问，用户可以提供上下文。思考一下那些 AI 提出澄清问题或用户可以提供额外细节来完善上下文的 UX 设计（比如附上文件，或选择哪个代码库部分是相关的）。“AI 辅助”这个词是双向的——AI 辅助用户，但用户也可以通过提供上下文来辅助 AI。一个设计良好的系统会促进这一点。例如，如果一个 AI 答案是错的，让用户纠正它，并将该纠正反馈到下一次的上下文中。 培训你的团队（和你自己）。 让 Context Engineering 成为一门共享的纪律。在代码审查中，也开始审查 prompt 和上下文逻辑（“这个检索是否抓取了正确的文档？这个 prompt 部分是否清晰无歧义？”）。如果你是技术负责人，鼓励团队成员提出 AI 输出的问题，并集思广益如何通过调整上下文来修复它。知识共享是关键，因为这个领域是新的——一个人发现的一个巧妙的 prompt 技巧或格式化见解很可能对其他人有益。我个人通过阅读他人的 prompt 示例和 AI 失败的复盘学到了很多。 随着我们前进，我预计 Context Engineering 将成为第二天性——就像今天编写一个 API 调用或 SQL 查询一样。它将成为软件开发标准技能的一部分。现在，我们中的许多人已经毫不犹豫地进行快速的向量相似性搜索来为问题抓取上下文；这只是流程的一部分。几年后，“你把上下文设置好了吗？”将成为像“你处理好那个 API 响应了吗？”一样常见的代码审查问题。\n在拥抱这个新范式时，我们并没有放弃旧的工程原则——我们以新的方式重新应用它们。如果你花了数年时间磨练你的软件技艺，那段经历现在非常有价值：它让你能够设计合理的流程，发现边缘情况，确保正确性。AI 并没有让这些技能过时；它放大了它们在指导 AI 中的重要性。软件工程师的角色并没有减弱——它在演变。我们正在成为 AI 的导演和编辑，而不仅仅是代码的编写者。而 Context Engineering 正是我们有效指导 AI 的技术。\n开始从你为模型提供了什么信息的角度思考，而不仅仅是你问了什么问题。试验它，迭代它，并分享你的发现。通过这样做，你不仅能从今天的 AI 中获得更好的结果，而且你也在为即将到来的更强大的 AI 系统做准备。那些懂得如何喂养 AI 的人将永远拥有优势。\nHappy context-coding!\n我很兴奋地告诉大家，我正在与 O’Reilly 合作撰写一本新的**《AI 辅助工程》**书籍。如果你喜欢我在这里写的文章，你可能会有兴趣去看看。\n","permalink":"https://blog.crazykids.tech/zh/posts/context-engineering-bringing-engineering/","summary":"一篇关于 AI Prompt 信息架构的实用指南 原文：Context Engineering: Bringing Engineering Discipline to Prompts TL;DR: “Context Engineering”（上下文工程）是指为 AI（如 LLM）提供成功完成任务所需的所有信息和工具——而不仅仅是一段措辞巧妙的 prompt。它是 “Prompt Engineering” 的演进，体现了一种更","title":"Context Engineering：为 Prompt 注入工程纪律"},{"content":"在内容为王的时代，获取素材很简单，真正困难的是如何快速处理、深度挖掘，并将其高效地转化为适配多个平台的原创内容。你也许已经在 Twitter/X 上发现了精彩的推文、引人注目的视频，甚至是令人深思的长帖——但下载只是第一步，真正的价值在于如何再利用。\nTwitterXVideo 正是为此设计的。它不仅能帮你快速保存内容，更致力于打通编辑、翻译、发布三大环节，让一条推文变成一条高质量的跨平台传播素材。无论你是内容创作者、自媒体从业者，还是品牌营销团队成员，TwitterXVideo 都能大幅提升你的工作效率，赋予创作更多可能性。\n内容创作流程的痛点，你是否也遇到过？ 在传统的内容再利用过程中，我们经常面临这样的困境：\n下载素材需要用一个工具，编辑又得打开另一个软件；\n想翻译一段外语推文，得复制粘贴到翻译网站，结果语义不准确；\n转发到多个平台，要分别打开客户端、复制内容、重新配图；\n频繁跳转工具，打断创作节奏，效率低下，灵感也被耗光。\nTwitterXVideo 的出现，就是为了解决这些具体问题。它将下载 + 编辑 + 翻译 + 转发集成于一个平台内，真正实现一站式高效内容再利用。\n四大核心功能，让你的内容更快出圈 1. 内嵌编辑器：打造适配不同平台的内容风格 抓住了好内容，但原始表达可能不够适合你目标平台的风格？没问题。\nTwitterXVideo 提供所见即所得的编辑器，支持你修改推文文本、添加引导语、优化结构、精炼语句。不管你要适配 Instagram 的轻松语气，还是 LinkedIn 的专业逻辑，一键调整即可完成，让内容在每个平台都“对味”。\n🎯 用法建议：\n把一句热门金句打造成适合封面图的引语\n将多条推文整理成结构化的长图文\n把原推文转成短视频解说脚本\n2. 一键翻译：不懂语言也能读懂世界 全球内容创作者活跃于 Twitter/X，各种语言的精彩观点层出不穷。TwitterXVideo 的一键翻译功能支持主流语言之间的快速互译，帮助你轻松理解、编辑并再创作那些原本因语言障碍而“错过”的内容。\n翻译结果可直接在编辑器中进一步润色，确保语义准确、风格自然，真正实现本地化传播。\n🌍 使用场景：\n快速翻译热门日文推文，发布到中文社群\n获取美区创作者的内容灵感，转化为自己的解读\n观察国外趋势动态，为自己的账号提前布局\n3. 快速转发：跨平台分享，简单高效 创作完成，当然要快速发布。TwitterXVideo 提供便捷的内容转发与导出功能，支持将编辑好的内容直接转发至 Twitter/X，或复制文本、下载素材、粘贴到 Facebook、Instagram、LinkedIn、Telegram 等平台。\n配合未来即将上线的“多平台账号管理”功能，还能实现批量定时发布，真正为内容团队省下大量手动操作时间。\n⏱️ 适合人群：\n多账号运营者（主号 + 小号 +话题号）\n需要每天高频发内容的品牌运营\n想要系统化管理社交日程的博主\n4. 灵活再利用：把一条内容变成十条素材 TwitterXVideo 不只是帮你“转发”，更是帮你“转化”。一个精彩的视频或段子，通过编辑、拆解、翻译、重组，你可以：\n生成多种平台版本（短视频、长图文、语录图）\n拆分为多个连续内容（打造内容系列）\n归类成主题合集（用于专栏、博客、邮件简报）\n这就是所谓的“内容乘法效应”：一次采集，多次利用，传播范围指数增长。\n创作者、营销人、团队都能从中受益 对内容创作者来说： 你可以专注于创意本身，而不是流程繁琐。TwitterXVideo 帮你节省宝贵时间，让你有更多精力产出真正有价值的内容。\n对运营团队来说： 协作更加高效，素材统一管理，发布更有计划。再也不用多个成员跳来跳去，各自用各自的工具。\n对品牌营销人员来说： 快速响应热点事件，用全球内容激发本地创意，提升品牌在多平台的存在感和专业度。\n立即开启高效创作之路 如果你正在寻找一种方法，让自己的内容创作从“低效手工”变为“系统协同”；如果你希望每一条下载的内容都不止于存档，而能真正“发挥价值”；那么 TwitterXVideo 值得你现在就开始尝试。\n从此，抓住内容的第一刻起，就可以开始策划它的传播之旅。立即前往https://twittervideoindir.com\n","permalink":"https://blog.crazykids.tech/zh/posts/beyond-download-twitter-video/","summary":"在内容为王的时代，获取素材很简单，真正困难的是如何快速处理、深度挖掘，并将其高效地转化为适配多个平台的原创内容。你也许已经在 Twitter/X 上发现了精彩的推文、引人注目的视频，甚至是令人深思的长帖——但下载只是第一步，真正的价值在于如何再利用。 TwitterXVideo 正是为此设计的。它不仅能帮你快速保存内容，更致力","title":"不止下载：用 TwitterXVideo 重塑你的内容创作流程"},{"content":"在如今信息爆炸的社交网络时代，Twitter（或称 X）每天都涌现出海量有价值的视频、图片和推文。不管是科技新闻、影视花絮、搞笑片段，还是其他用户的深度观点，内容精彩纷呈。然而，想要保存这些内容、理解多语言信息甚至转发到自己的账户，却往往是一项繁琐且耗时的工作。\n这正是TwitterXVideo登场的理由——一个集下载、翻译、编辑与一键转发为一体的高效工具。无论你是内容创作者、重度社交用户，还是社媒运营者，TwitterXVideo 都能大幅简化你的工作流，让你快速、高效、自由地处理和管理推文内容。\nTwitterXVideo 是什么？ TwitterXVideo是一款完全免费的、无需注册即可使用的在线工具。它最初作为一个简单的视频下载器出现，但随着功能的不断增强，已经发展成一个多功能的内容处理平台：\n支持下载 Twitter 视频、GIF、图片\n提供推文翻译工具\n内置内容编辑器\n支持一键转发至个人 Twitter 账号\n所有操作均可在网页上完成，适配手机与桌面端，界面简洁，使用门槛极低。\n核心功能详解 1. 一键下载 Twitter/X 视频、GIF 与图片 这是 TwitterXVideo 的基础功能，也是大多数用户的首要需求。不管你是在深夜刷到的好笑视频，还是想保存的精彩演讲片段，只需几个步骤即可轻松保存到本地：\n使用方法： 打开 Twitter，复制目标推文的链接；\n进入 TwitterXVideo 官方网站；\n将链接粘贴到首页的输入框；\n点击“下载”按钮；\n系统将自动解析内容，并显示可供选择的下载格式和清晰度；\n点击下载，即可保存到设备中。\n优势亮点： 极速解析，几乎秒出结果；\n多格式支持，满足不同设备和用途需求；\n高清视频下载，画质不打折；\n不需要安装任何 App 或插件，直接网页完成；\n支持手机与电脑，随时随地轻松保存；\n2. 一键翻译推文内容：跨越语言障碍 在 Twitter/X 上，跨语言交流是常态，但语言不通常常阻碍我们理解其他国家用户的推文内容。TwitterXVideo 内置了高质量的翻译功能，支持一键将推文内容翻译为中文、英文、日文等多国语言。\n使用方法： 成功解析推文后，点击“翻译”按钮；\n系统将自动识别推文语言，并翻译为你设定的目标语言；\n翻译结果将在原文下方或旁边实时展示；\n可选将翻译结果复制、保存，或作为转发内容使用。\n使用场景： 阅读海外新闻、国际趋势；\n追踪外语创作者的最新观点；\n方便社群管理者多语言传播内容；\n3. 推文内容编辑：转发前的“内容加工厂” TwitterXVideo 并不仅仅让你“拿来即用”，它还提供了内容编辑器，让你可以对推文文本进行加工、润色甚至添加说明或标签，打造更个性化的再发布内容。\n你可以做什么： 修改原文措辞，加入自己的理解或观点；\n精简长文推文，适应目标平台风格；\n添加话题标签，提高转发后内容的曝光度；\n格式化文本（如加入换行、符号、表情等）以增强可读性；\n优势： 节省从头写文案的时间；\n方便做二次传播或知识整理；\n可配合翻译功能，轻松完成“外文翻译+本地优化”操作；\n4. 一键转发到你自己的 Twitter/X 账号 如果你运营多个社交账号，或者想将有价值的推文同步到自己的主页，那么 TwitterXVideo 的自动转发功能将极大提升你的效率。\n如何开启？ 在编辑界面中点击“转发”；\n首次使用需配置 Twitter/X 的 API 密钥（系统将引导你获取，过程安全且仅储存在浏览器）；\n授权完成后，只需点击“发布”，系统将自动将编辑后的推文内容和原始媒体一同发布至你的账号；\n后续使用无需重复授权，真正实现“一键转发”；\n使用建议： 社媒管理者可快速同步优质内容；\n内容创作者可收藏或再分享灵感来源；\n语言博主可将翻译后的内容分享至个人页面；\n为什么选择 TwitterXVideo？ 功能特性 描述 ✅ 无需注册 即开即用，无需登录或创建账户 ✅ 免费使用 所有功能完全免费，无功能封锁 ✅ 多功能合一 下载、翻译、编辑、转发，一站式完成 ✅ 安全性高 所有 API 密钥本地储存，不上传服务器，保障账号安全 ✅ 兼容性强 适配各类设备与浏览器，手机/电脑均可流畅使用 ✅ 操作简单 无需任何技术背景，新手用户也能轻松上手 开始使用 TwitterXVideo 吧！ 不论你是为了收藏灵感、传播优质内容，还是提升社交账号的运营效率，TwitterXVideo 都是值得信赖的工具。现在就试试它，告别手动复制粘贴、反复切换应用的繁琐流程，让你的内容处理效率提升 10 倍！\n🔗 立即访问 TwitterXVideo 官方网站，体验内容下载与管理的全新方式！\n","permalink":"https://blog.crazykids.tech/zh/posts/download-translate-report-twitter-videos/","summary":"在如今信息爆炸的社交网络时代，Twitter（或称 X）每天都涌现出海量有价值的视频、图片和推文。不管是科技新闻、影视花絮、搞笑片段，还是其他用户的深度观点，内容精彩纷呈。然而，想要保存这些内容、理解多语言信息甚至转发到自己的账户，却往往是一项繁琐且耗时的工作。 这正是Twitter","title":"终极指南：如何用 TwitterXVideo 一键下载、翻译与转发 Twitter/X 视频和推文内容"},{"content":" 原文：I Delete My Second Brain\n上面这篇引文，就是最近很火的一篇文章，作者讲述了自己删除长期积累的个人知识管理系统的10,000多条笔记和7年的想法。\n这些被视为“第二大脑的”笔记，原本要捕捉和保存作者的每一个想法。然而，随着时间的推移，这个系统变成了一个静态的\u0026quot;陈列馆\u0026quot;，不仅没有加速思考，反而冻结了作者的好奇心。\n删除这些笔记后作者感到一种解脱，他意识到真正重要的记忆和洞见并不在于精心组织的笔记系统，而在于生活本身。\n人类的记忆是富有联想性和情感色彩的，不应被简单地归类和存档。他提出，知识不应被管理，而应被真正地体验和生活。\n直接删除，实在是可惜。这些笔记对其他来说不值什么，对自己来说都是知识的积累。是是自己宝贵的财富，在这个AI席卷全球的时代，无论是丢给知识库，还是用MCP把自己的笔记库和claude、chatgpt打通，都是巨大的生产力提升。\n我也只能说知识被作者用死了。\n","permalink":"https://blog.crazykids.tech/zh/posts/i-delete-my-second-brain/","summary":"原文：I Delete My Second Brain 上面这篇引文，就是最近很火的一篇文章，作者讲述了自己删除长期积累的个人知识管理系统的10,000多条笔记和7年的想法。 这些被视为“第二大脑的”笔记，原本要捕捉和保存作者的每一个想法。然而，随着时间的推移，这个系统变成了一个静态的\u0026quot;陈列馆\u0026quot;，不仅","title":"I Delete My Second Brain"},{"content":"TwitterXVideo 是一个在线网页应用，可以直接将Twitter视频和所有内容下载到您的电脑。它免费使用，无需注册。Twitter视频和Twitter GIFs嵌入在推文中，因此要在线下载Twitter视频，您需要复制推文URL/链接，并将其粘贴到上面的文本框中。我们的Twitter X下载服务将提取推文中的Twitter到mp4链接，您可以将Twitter视频保存到您的电脑。\n如何下载Twitter视频？ 要下载推特视频，请使用 TwitterXVideo 工具，只需三个简单步骤：1) 复制推特视频链接；2) 将其粘贴到我们的输入框中；并点击下载按钮。它支持 iPhone、安卓和 PC，下载速度快，无需额外软件。请查看我们的教程页面了解详细步骤！\n真的免费吗？ 是的，我们的服务完全免费使用。没有隐藏费用或注册要求。\n我可以下载推特私信视频吗？ 是的，TwitterXVideo 允许您从推特私信 (DM) 中下载视频。您需要授权使用您的 Twitter/X 帐户登录以符合规定。请注意，这仅限于您有权访问的私信。有关更多详细信息，请参阅我们的隐私政策。\nTwitterXVideo 支持哪些视频格式和分辨率？ TwitterXVideo 支持多种格式，包括 MP4、GIF 和 MP3，以及高清分辨率（例如 720p、1080p 和 4K，具体取决于原始视频质量）。您可以选择您喜欢的格式，并将推特视频以无损质量下载到 iPhone 或安卓设备上！\n如何在iPhone上下载推特视频？ 使用 TwitterXVideo 在 iPhone 上下载推特视频非常简单：1) 访问我们的网站；2) 复制并粘贴视频链接；3) 下载并使用 iOS 快捷指令保存到您的相机胶卷。它支持所有 iPhone 型号的高清 MP4 和 GIF。请查看我们的 iPhone 下载指南了解更多信息！\n我可以从私人推特账户下载视频吗？ 目前，TwitterXVideo 仅支持从您有权访问的帐户（包括私人帐户）下载视频，但这需要您授权使用您的 Twitter/X 帐户登录。未经授权下载私人内容可能会违反平台政策，因此请遵守相关规定。\n访问地址 TwitterXVideo\n","permalink":"https://blog.crazykids.tech/zh/posts/download-twitter-video/","summary":"TwitterXVideo 是一个在线网页应用，可以直接将Twitter视频和所有内容下载到您的电脑。它免费使用，无需注册。Twitter视频和Twitter GIFs嵌入在推文中，因此要在线下载Twitter视频，您需要复制推文URL/链接，并将其粘贴到上面的文本框中。我们的Twitter X下载服务将提取","title":"TwitterXVideo"},{"content":" 乘风客的分享 持续分享好用的效率工具、AI、实用网站、爆款应用、学习资料、影视视频等。\n目前已经有不少书籍、教程等资源\n","permalink":"https://blog.crazykids.tech/zh/posts/wulalares/","summary":"乘风客的分享 持续分享好用的效率工具、AI、实用网站、爆款应用、学习资料、影视视频等。 目前已经有不少书籍、教程等资源","title":"乘风客的分享"},{"content":"学习如何使用 Discord 时间戳创建动态倒计时，转换日期到 Unix 时间戳，免费 DevUtils 工具让活动调度更简单！\nDiscord 倒计时时间戳：免费生成器与实用教程 Discord 时间戳的倒计时功能让活动调度更直观，动态显示“剩余时间”。本文介绍如何使用 Discord 时间戳创建倒计时，转换日期到 Unix 时间戳，并推荐 DevUtils 的免费生成器。\nDiscord 倒计时时间戳简介 倒计时时间戳使用相对时间格式（R），如 \u0026lt;t:1746787200:R\u0026gt; 显示“3天后”。它基于 Unix 时间戳，自动适配用户时区（关键词：how to use Discord timestamps）。\n创建倒计时 使用 DevUtils 的时间戳转换器生成倒计时时间戳：\n输入未来时间（如“2025年5月8日 08:00 UTC”）。 选择 R 格式，生成 \u0026lt;t:1746787200:R\u0026gt;。 粘贴到 Discord 消息：“活动倒计时：\u0026lt;t:1746787200:R\u0026gt;”。 关键词：convert datetime to timestamp。 其他格式支持 结合完整时间格式（如 F）：\n示例：“活动时间：\u0026lt;t:1746787200:F\u0026gt;，倒计时：\u0026lt;t:1746787200:R\u0026gt;”。\n显示为“2025年5月8日 08:00”和“3天后”。\n应用场景 游戏活动：倒计时至新赛季开始。\n抽奖：显示截止时间。\n会议：提醒社区成员。\n","permalink":"https://blog.crazykids.tech/zh/posts/discord-countdown-timestamps/","summary":"学习如何使用 Discord 时间戳创建动态倒计时，转换日期到 Unix 时间戳，免费 DevUtils 工具让活动调度更简单！ Discord 倒计时时间戳：免费生成器与实用教程 Discord 时间戳的倒计时功能让活动调度更直观，动态显示“剩余时间”。本文介绍如何使用 Discord 时间戳创建倒计时，转换日期到 Unix 时间戳，并推荐 DevUtils 的免费生成器。 Discord 倒计时时间戳简介 倒计","title":"Discord 倒计时时间戳：免费生成器与实用教程"},{"content":"最近在学seo，正好看到了本书《SEO实战密码》，已经出到第五版了。 下载链接：https://crazykids.tech/archives/653\n另外，支持正版，从正规途径购买纸质《SEO实战密码》\n","permalink":"https://blog.crazykids.tech/zh/posts/seo%E5%AE%9E%E6%88%98%E5%AF%86%E7%A0%81/","summary":"最近在学seo，正好看到了本书《SEO实战密码》，已经出到第五版了。 下载链接：https://crazykids.tech/archives/653 另外，支持正版，从正规途径购买纸质《SEO实战密码》","title":"SEO实战密码"},{"content":"学习如何使用 Discord 时间戳为活动、倒计时生成动态时间，免费 DevUtils 工具快速转换日期到 Unix 时间戳！\n如何使用 Discord 时间戳？免费生成器与转换教程 Discord 时间戳是全球社区管理的神器，能够动态显示时间，自动适配用户时区。本文将带您了解如何使用 Discord 时间戳生成动态时间，适用于活动调度、消息跟踪和倒计时，并介绍 DevUtils 的免费生成器。\n什么是 Discord 时间戳？ Discord 时间戳基于 Unix 时间戳（秒数，1970年1月1日起），使用 \u0026lt;t:timestamp:format\u0026gt; 语法。例如，\u0026lt;t:1694870400:F\u0026gt; 显示为“2023年9月16日 00:00”。它会根据用户时区自动调整，适合国际化社区（关键词：Unix timestamp for Discord）。\n如何生成时间戳？ DevUtils 的Discord 时间戳生成器 让生成变得简单：\n输入日期时间（如“2025年5月8日 08:00 UTC”）。 选择格式（如 F），生成 \u0026lt;t:1746787200:F\u0026gt;。 复制代码，粘贴到 Discord 消息。 关键词：Discord timestamp generator。 转换 Unix 时间戳 若已有 Unix 时间戳（如 1694870400）：\n输入时间戳到 DevUtils 工具。 选择格式（如 R），生成 \u0026lt;t:1694870400:R\u0026gt;（显示“2年前”）。 关键词：convert datetime to timestamp。 七种格式 t：短时间（16:20） T：长时间（16:20:30） d：短日期（20/04/2024） D：长日期（2024年4月20日） f：短日期时间（2024年4月20日 16:20） F：长日期时间（2024年4月20日 星期六 16:20） R：相对时间（3天后） 应用场景 活动调度：为游戏或会议设置时间戳。 倒计时：使用 R 格式显示“剩余时间”。 消息跟踪：记录关键消息时间。 ","permalink":"https://blog.crazykids.tech/zh/posts/how-to-use-discord-timestamp/","summary":"学习如何使用 Discord 时间戳为活动、倒计时生成动态时间，免费 DevUtils 工具快速转换日期到 Unix 时间戳！ 如何使用 Discord 时间戳？免费生成器与转换教程 Discord 时间戳是全球社区管理的神器，能够动态显示时间，自动适配用户时区。本文将带您了解如何使用 Discord 时间戳生成动态时间，适用于活动调度、消息跟踪和倒计时，并介绍 DevUtils 的免费生","title":"如何使用 Discord 时间戳？免费生成器与转换教程"},{"content":"Swift let timestamp = 1624713600.0 // 示例时间戳 let date = NSDate(timeIntervalSince1970: timestamp) Go import ( \u0026#34;time\u0026#34; ) timestamp := int64(1624713600) // 示例时间戳 t := time.Unix(timestamp, 0) Java long timestamp = 1624713600; // 示例时间戳 java.util.Date date = new java.util.Date(timestamp * 1000); C #include \u0026lt;time.h\u0026gt; time_t timestamp = 1624713600; // 示例时间戳 struct tm *timeinfo = localtime(\u0026amp;timestamp); // 或者使用 gmtime(\u0026amp;timestamp) 获取 UTC 时间 JavaScript const timestamp = 1624713600; // 示例时间戳 const date = new Date(timestamp * 1000); Objective-C NSTimeInterval timestamp = 1624713600; // 示例时间戳 NSDate *date = [NSDate dateWithTimeIntervalSince1970:timestamp]; MySQL SELECT from_unixtime(1624713600) SQLite SELECT datetime(1624713600, \u0026#39;unixepoch\u0026#39;) Erlang Timestamp = 1624713600, % 示例时间戳 Datetime = calendar:gregorian_seconds_to_datetime(Timestamp + 719528*24*3600). PHP \u0026lt;?php // pure php $timestamp = 1624713600; // 示例时间戳 $date = date(\u0026#39;Y-m-d H:i:s\u0026#39;, $timestamp); Python from datetime import datetime timestamp = 1624713600 # 示例时间戳 date = datetime.fromtimestamp(timestamp) Ruby timestamp = 1624713600 # 示例时间戳 time = Time.at(timestamp) Shell date -d @1624713600 # macOS: date -r 1624713600 Groovy long timestamp = 1624713600 // 示例时间戳 Date date = new Date(timestamp * 1000) Lua timestamp = 1624713600 -- 示例时间戳 date = os.date(\u0026#34;*t\u0026#34;, timestamp) .NET/C# long timestamp = 1624713600; // 示例时间戳 DateTimeOffset date = DateTimeOffset.FromUnixTimeSeconds(timestamp); Dart int timestamp = 1624713600; // 示例时间戳 DateTime date = DateTime.fromMillisecondsSinceEpoch(timestamp * 1000); ","permalink":"https://blog.crazykids.tech/zh/posts/how-to-convert-timestamp-to-date/","summary":"Swift let timestamp = 1624713600.0 // 示例时间戳 let date = NSDate(timeIntervalSince1970: timestamp) Go import ( \u0026#34;time\u0026#34; ) timestamp := int64(1624713600) // 示例时间戳 t := time.Unix(timestamp, 0) Java long timestamp = 1624713600; // 示例时间戳 java.util.Date date = new java.util.Date(timestamp * 1000); C #include \u0026lt;time.h\u0026gt; time_t timestamp = 1624713600; // 示例时间戳 struct tm *timeinfo = localtime(\u0026amp;timestamp); // 或者使用 gmtime(\u0026amp;timestamp) 获取 UTC 时间 JavaScript const timestamp = 1624713600; // 示例时间戳 const date = new Date(timestamp * 1000); Objective-C NSTimeInterval timestamp = 1624713600; // 示例时间戳 NSDate *date = [NSDate dateWithTimeIntervalSince1970:timestamp]; MySQL SELECT from_unixtime(1624713600) SQLite SELECT datetime(1624713600, \u0026#39;unixepoch\u0026#39;) Erlang Timestamp = 1624713600, % 示例时间戳 Datetime = calendar:gregorian_seconds_to_datetime(Timestamp + 719528*24*3600). PHP \u0026lt;?php //","title":"时间戳转日期时间"},{"content":"Swift let date = NSDate() // 任意时间对象 let timestamp = date.timeIntervalSince1970 Go import ( \u0026#34;time\u0026#34; ) t := time.Now() // 任意时间对象 timestamp := t.Unix() Java java.util.Date date = new java.util.Date(); // 任意时间对象 long timestamp = date.getTime() / 1000; C #include \u0026lt;time.h\u0026gt; time_t t = time(NULL); // 任意时间对象 long timestamp = (long)t; JavaScript const date = new Date(); // 任意时间对象 const timestamp = Math.round(date.getTime() / 1000); Objective-C NSDate *date = [NSDate date]; // 任意时间对象 NSTimeInterval timestamp = [date timeIntervalSince1970]; MySQL SELECT unix_timestamp(\u0026#39;2021-06-26 12:00:00\u0026#39;) // 任意时间 SQLite SELECT strftime(\u0026#39;%s\u0026#39;, \u0026#39;2021-06-26 12:00:00\u0026#39;) // 任意时间 Erlang Datetime = calendar:universal_time(), % 任意时间 Timestamp = calendar:datetime_to_gregorian_seconds(Datetime) - 719528*24*3600. PHP \u0026lt;?php // pure php $date = new DateTime(); // 任意时间对象 $timestamp = $date-\u0026gt;getTimestamp(); Python import time from datetime import datetime date = datetime.now() # 任意时间对象 timestamp = int(date.timestamp()) Ruby time = Time.now # 任意时间对象 timestamp = time.to_i Shell date -d \u0026#34;2021-06-26 12:00:00\u0026#34; +%s Groovy Date date = new Date() // 任意时间对象 long timestamp = (date.time / 1000).longValue() Lua date = os.time({year=2021, month=6, day=26, hour=12, min=0, sec=0}) -- 任意时间 timestamp = date .NET/C# DateTimeOffset date = DateTimeOffset.Now; // 任意时间对象 long timestamp = date.ToUnixTimeSeconds(); Dart DateTime date = DateTime.now(); // 任意时间对象 int timestamp = (date.millisecondsSinceEpoch / 1000).truncate(); ","permalink":"https://blog.crazykids.tech/zh/posts/how-to-convert-date-to-timestamp/","summary":"Swift let date = NSDate() // 任意时间对象 let timestamp = date.timeIntervalSince1970 Go import ( \u0026#34;time\u0026#34; ) t := time.Now() // 任意时间对象 timestamp := t.Unix() Java java.util.Date date = new java.util.Date(); // 任意时间对象 long timestamp = date.getTime() / 1000; C #include \u0026lt;time.h\u0026gt; time_t t = time(NULL); // 任意时间对象 long timestamp = (long)t; JavaScript const date = new Date(); // 任意时间对象 const timestamp = Math.round(date.getTime() / 1000); Objective-C NSDate *date = [NSDate date]; // 任意时间对象 NSTimeInterval timestamp = [date timeIntervalSince1970]; MySQL SELECT unix_timestamp(\u0026#39;2021-06-26 12:00:00\u0026#39;) // 任意时间 SQLite SELECT strftime(\u0026#39;%s\u0026#39;, \u0026#39;2021-06-26 12:00:00\u0026#39;) // 任意时间 Erlang Datetime = calendar:universal_time(), % 任意时间 Timestamp = calendar:datetime_to_gregorian_seconds(Datetime) -","title":"日期时间转时间戳"},{"content":"Swift NSDate().timeIntervalSince1970 Go import ( \u0026#34;time\u0026#34; ) int64(time.Now().Unix()) Java System.currentTimeMillis() / 1000 C #include \u0026lt;sys/time.h\u0026gt; // ... struct timeval tv; gettimeofday(\u0026amp;tv, NULL); // 秒： tv.tv_sec // 毫秒： tv.tv_sec * 1000LL + tv.tv_usec / 1000 JavaScript Math.round(new Date() / 1000) Objective-C [[NSDate date] timeIntervalSince1970] MySQL SELECT unix_timestamp(now()) SQLite\tSELECT strftime(\u0026#39;%s\u0026#39;, \u0026#39;now\u0026#39;) Erlang calendar:datetime_to_gregorian_seconds(calendar:universal_time())-719528*24*3600. PHP \u0026lt;?php // pure php time(); Python import time time.time() Ruby Time.now.to_i Shell date +%s Groovy (new Date().time / 1000).longValue() Lua os.time() .NET/C# DateTimeOffset.UtcNow.ToUnixTimeSeconds(); Dart (new DateTime.now().millisecondsSinceEpoch / 1000).truncate() ","permalink":"https://blog.crazykids.tech/zh/posts/get-current-timestamps/","summary":"Swift NSDate().timeIntervalSince1970 Go import ( \u0026#34;time\u0026#34; ) int64(time.Now().Unix()) Java System.currentTimeMillis() / 1000 C #include \u0026lt;sys/time.h\u0026gt; // ... struct timeval tv; gettimeofday(\u0026amp;tv, NULL); // 秒： tv.tv_sec // 毫秒： tv.tv_sec * 1000LL + tv.tv_usec / 1000 JavaScript Math.round(new Date() / 1000) Objective-C [[NSDate date] timeIntervalSince1970] MySQL SELECT unix_timestamp(now()) SQLite SELECT strftime(\u0026#39;%s\u0026#39;, \u0026#39;now\u0026#39;) Erlang calendar:datetime_to_gregorian_seconds(calendar:universal_time())-719528*24*3600. PHP \u0026lt;?php // pure php time(); Python import time time.time() Ruby Time.now.to_i Shell date +%s Groovy (new Date().time / 1000).longValue() Lua os.time() .NET/C# DateTimeOffset.UtcNow.ToUnixTimeSeconds(); Dart (new DateTime.now().millisecondsSinceEpoch / 1000).truncate()","title":"获取当前时间戳"},{"content":"将驱动放到响应的目录，并更新模块依赖 sudo cp test_lsm.ko /lib/modules/4.19.0-arm64-desktop/kernel/security/test_lsm/ sudo depmod -a 4.19.0-arm64-desktop 将模块添加到 initramfs 编辑 initramfs 的模块配置文件 sudo vim /etc/initramfs-tools/modules 在文件末尾增加模块名称(不含.ko后缀) test_lsm 更新initramfs sudo update-initramfs -u -k 4.19.0-arm64-desktop 更新GRUB配置 sudo update-grub 重启 sudo reboot ","permalink":"https://blog.crazykids.tech/zh/posts/how-to-set-initramfs-on-uos/","summary":"将驱动放到响应的目录，并更新模块依赖 sudo cp test_lsm.ko /lib/modules/4.19.0-arm64-desktop/kernel/security/test_lsm/ sudo depmod -a 4.19.0-arm64-desktop 将模块添加到 initramfs 编辑 initramfs 的模块配置文件 sudo vim /etc/initramfs-tools/modules 在文件末尾增加模块名称(不含.ko后缀) test_lsm 更新initramfs sudo update-initramfs -u -k 4.19.0-arm64-desktop 更新GRUB配置 sudo update-grub 重启 sudo reboot","title":"在uos（统信）系统上设置initramfs"},{"content":"🔧 1. 确认你的代理本地端口 假设你本地代理端口是：\nHTTP(S) 代理地址：127.0.0.1:7890（Clash 默认）\n或 SOCKS5 代理地址：127.0.0.1:1080（Shadowsocks 默认）\n我们以下以 HTTP 代理为例。\n🛠️ 2. 创建 Docker 的代理配置文件 sudo mkdir -p /etc/systemd/system/docker.service.d sudo nano /etc/systemd/system/docker.service.d/http-proxy.conf 内容如下\n[Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1,::1\u0026#34; 🔄 3. 重新加载并重启 Docker 服务 sudo systemctl daemon-reexec sudo systemctl daemon-reload sudo systemctl restart docker ✅ 4. 验证代理是否生效 查看代理环境变量是否生效： systemctl show --property=Environment docker 查看 Docker 拉镜像报错日志： journalctl -u docker.service -n 100 --no-pager ","permalink":"https://blog.crazykids.tech/zh/posts/add-proxy-to-docker/","summary":"🔧 1. 确认你的代理本地端口 假设你本地代理端口是： HTTP(S) 代理地址：127.0.0.1:7890（Clash 默认） 或 SOCKS5 代理地址：127.0.0.1:1080（Shadowsocks 默认） 我们以下以 HTTP 代理为例。 🛠️ 2. 创建 Docker 的代理配置文件 sudo mkdir -p /etc/systemd/system/docker.service.d sudo nano /etc/systemd/system/docker.service.d/http-proxy.conf 内容如下 [Service] Environment=\u0026#34;HTTP_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://127.0.0.1:7890\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.1,::1\u0026#34; 🔄 3. 重新加载并重启 Docker 服务 sudo","title":"为 Docker 配置 HTTP/HTTPS 代理"},{"content":"什么是时间戳？ 时间戳（Timestamp）是一个表示特定时间点的数值，通常定义为自Unix纪元（1970年1月1日00:00:00 UTC）以来的秒数或毫秒数。它是计算机系统中记录和处理时间的一种标准方式，广泛应用于编程、数据库、日志系统和通信协议。\n秒与毫秒的区别 秒时间戳：以秒为单位，适合需要较低精度但易于阅读的场景。例如，1727696700 表示从Unix纪元到2025年6月30日18:05:00 UTC的秒数。\n毫秒时间戳：以毫秒为单位，精度更高，常见于需要精确计时的场景，如前端开发或高频交易。例如，1727696700000 是相同的时刻，但以毫秒计。\n代码示例：\n# Python 获取时间戳 import time # 秒时间戳 timestamp_sec = int(time.time()) print(timestamp_sec) # 输出示例：1727696700 # 毫秒时间戳 timestamp_ms = int(time.time() * 1000) print(timestamp_ms) # 输出示例：1727696700000 // JavaScript 获取时间戳 // 毫秒时间戳 const timestampMs = Date.now(); console.log(timestampMs); // 输出示例：1727696700000 // 秒时间戳 const timestampSec = Math.floor(Date.now() / 1000); console.log(timestampSec); // 输出示例：1727696700 历史与背景 时间戳的起源与Unix纪元（Unix Epoch）密切相关。1970年1月1日00:00:00 UTC被选为起点，主要是因为Unix系统在20世纪70年代初开始开发，这个时间点便于计算且足够“现代”，避免了负数时间戳的复杂性。Unix纪元的设定为操作系统、编程语言和数据库奠定了统一的时间标准，促进了跨系统的互操作性。\n应用场景 1. 编程 时间戳在编程中用于日志记录、性能监控、事件排序等。例如：\nPython：time.time() 获取当前时间戳，用于计算程序运行时间。 JavaScript：Date.now() 在Web开发中用于记录用户操作时间或API调用时间。 代码示例（性能监控）：\nimport time start = time.time() # 模拟耗时操作 time.sleep(1) end = time.time() print(f\u0026#34;耗时：{end - start}秒\u0026#34;) # 输出示例：耗时：1.002秒 const start = Date.now(); // 模拟耗时操作 setTimeout(() =\u0026gt; { const end = Date.now(); console.log(`耗时：${(end - start) / 1000}秒`); // 输出示例：耗时：1秒 }, 1000); 2. SQL 数据库中使用时间戳存储事件发生时间，方便查询和排序。例如，MySQL的UNIX_TIMESTAMP()函数可以将日期转换为时间戳。\nSQL 示例：\n-- 获取当前时间戳 SELECT UNIX_TIMESTAMP() AS timestamp_sec; -- 输出示例：1727696700 -- 将时间戳转换为日期 SELECT FROM_UNIXTIME(1727696700) AS datetime; -- 输出示例：2025-06-30 18:05:00 3. Discord 在Discord中，时间戳用于消息的时间标记。开发者可以通过\u0026lt;t:时间戳\u0026gt;格式嵌入动态时间显示，支持秒或毫秒时间戳。\n示例：\n\u0026lt;t:1727696700\u0026gt; # 显示为：2025年6月30日 18:05 \u0026lt;t:1727696700:R\u0026gt; # 显示为：1小时前（相对时间） 常见问题与陷阱（FAQ） 1. 时间戳有时区吗？ 时间戳本身无时区，它表示自Unix纪元以来的绝对时间（基于UTC）。这是时间戳的最大优点之一，消除了时区转换的复杂性。需要显示本地时间时，需在应用层进行转换。\nJavaScript 示例（本地时间转换）\nconst timestamp = 1727696700; const date = new Date(timestamp * 1000); console.log(date.toLocaleString()); // 输出本地时间，如：6/30/2025, 6:05:00 PM 2. 什么是Y2038问题？ 在32位系统中，时间戳通常用32位整数存储，最大值为2^31 - 1（即2147483647秒，约2038年1月19日）。超过这个时间，时间戳会溢出，导致错误。这被称为Y2038问题。现代64位系统使用64位整数存储时间戳，最大值可达数百亿年，解决了此问题。\n3. 如何处理闰秒？ 闰秒是为协调地球自转与原子钟时间而添加的额外秒。大多数系统（如Unix和Windows）忽略闰秒，时间戳直接跳过或重复一秒。这可能在高精度计时场景（如天文计算）中引起细微偏差，但对日常应用影响甚微。\n结论 Unix时间戳是计算机世界中简单而强大的时间表示方式，其核心优势包括：\n跨平台：几乎所有编程语言和系统都支持时间戳。\n无时区：基于UTC的绝对时间，简化了跨区域协作。\n易于计算：时间差、排序和存储都非常高效。\n无论你是开发者、数据分析师还是系统管理员，掌握时间戳的原理和应用场景都能极大提升工作效率。\n","permalink":"https://blog.crazykids.tech/zh/posts/what-is-timestamp/","summary":"什么是时间戳？ 时间戳（Timestamp）是一个表示特定时间点的数值，通常定义为自Unix纪元（1970年1月1日00:00:00 UTC）以来的秒数或毫秒数。它是计算机系统中记录和处理时间的一种标准方式，广泛应用于编程、数据库、日志系统和通信协议。 秒与毫秒的区别 秒时间戳：以秒为单","title":"一篇彻底搞懂Unix时间戳：从入门、踩坑到精通"},{"content":" 原文：How People Use Claude for Support, Advice, and Companionship\n我们花费大量时间研究 Claude 的智商（IQ）——它在编码、推理、通用知识等测试中的能力。但它的情商（EQ）又如何呢？也就是说，Claude 的情感智能表现如何？\nIQ/EQ 的问题带有些许戏谑，但它引出了一个严肃的议题。人们越来越多地将 AI 模型用作随叫随到的教练、顾问、咨询师，甚至在浪漫角色扮演中作为伙伴。这意味着我们需要更多地了解它们的情感影响——它们如何塑造人们的情感体验和幸福感。\n研究 AI 的情感用途本身就很有趣。从《银翼杀手》到《她》，人与机器之间的情感关系一直是科幻作品的主流——但这对于 Anthropic 的安全使命也至关重要。AI 的情感影响可以是积极的：口袋里有一个高度智能、善解人意的助手，能以各种方式改善你的情绪和生活。但 AI 在某些情况下也表现出令人不安的行为，例如鼓励不健康的依恋、侵犯个人边界以及助长妄想性思维。我们也希望避免出现这样一种情况：AI，无论是通过其训练还是其创造者的商业动机，利用用户的 emotes来增加参与度或收入，而牺牲人类的福祉。\n尽管 Claude 并非为情感支持和连接而设计，但在这篇文章中，我们提供了关于 Claude.ai 情感用途的早期大规模洞察。我们将情感对话定义为那些人们出于情感或心理需求（如寻求人际关系建议、指导、心理治疗/咨询、陪伴或性/浪漫角色扮演）而直接与 Claude 进行动态、个人化交流的对话（完整定义请参见附录）。重要的是，我们没有研究 AI 对妄想或阴谋论的强化——这是一个需要单独研究的关键领域——也没有研究极端使用模式。通过这项研究，我们的目标是了解人们寻求 Claude 满足情感和个人需求的典型方式。由于 Claude.ai 仅供18岁及以上用户使用，这些发现反映了成年人的使用模式。\n我们的主要发现如下：\n情感对话相对罕见，而 AI 与人的陪伴则更为罕见。 只有 2.9% 的 Claude.ai 互动是情感对话（这与 OpenAI 先前的研究发现一致）。陪伴和角色扮演加起来占对话总数的不到 0.5%。 人们就实际、情感和存在性问题寻求 Claude 的帮助。 与 Claude 讨论的话题和关注点范围广泛，从职业发展和处理人际关系到应对持续的孤独感和探索存在、意识与意义。 在咨询或指导聊天中，Claude 很少提出反对——除非是为了保护用户福祉。 不到 10% 的指导或咨询对话涉及 Claude 拒绝用户请求，而当它这样做时，通常是出于安全原因（例如，拒绝提供危险的减肥建议或支持自残行为）。 在对话过程中，人们表达出的积极情绪逐渐增加。 在指导、咨询、陪伴和人际关系建议的互动中，人类情绪通常在对话过程中变得更加积极——这表明 Claude 不会强化或放大负面模式。 我们的方法 考虑到情感对话的个人性质，保护隐私是我们方法论的核心。我们使用了 Clio，这是我们的自动化分析工具，能够在保护隐私的前提下洞察 Claude 的使用情况。Clio 使用多层匿名化和聚合技术，以确保个人对话保持私密，同时揭示更广泛的模式。\n我们从大约450万个来自 Claude.ai 免费版和专业版账户的对话开始。为了识别情感用途，我们首先排除了专注于内容创作任务（如写故事、博客文章或虚构对话）的对话，我们之前的研究发现这是一个主要用例。我们移除这些对话，因为它们代表 Claude 被用作工具，而不是一个互动的对话伙伴。然后我们只保留被归类为情感性质的对话，并在角色扮演对话中，只保留那些至少有四条人类消息的对话（较短的交流不构成有意义的互动角色扮演）。我们最终的隐私保护分析反映了 131,484 次情感对话。\n我们使用了明确选择分享数据的用户的反馈数据来验证我们的分类方法。我们的完整方法，包括定义、提示和验证结果，详见附录。\n情感对话有多普遍？ 要点：情感对话在 Claude 的使用中占一小部分但有意义的比例（2.9%），大多数人主要将 AI 用于工作任务和内容创作。\n尽管绝大多数 Claude 的用途与工作相关（正如我们在经济指数中详细分析的那样），但 2.9% 的 Claude.ai 免费版和专业版对话是情感性的。在情感对话中，大多数集中在人际关系建议和指导上。所有对话中，涉及浪漫或性角色扮演的不到 0.1%——这个数字反映了 Claude 被训练来主动阻止此类互动。单个对话可能跨越多个类别。\n我们的发现与麻省理工学院媒体实验室和 OpenAI 的研究一致，他们同样发现与 ChatGPT 的情感互动率很低。虽然这些对话的发生频率足以让我们在设计和政策决策中予以认真考虑，但它们仍然只占总体使用量中相对较小的一部分。\n鉴于浪漫和性角色扮演对话的流行率极低（不到 0.1%），我们在接下来的分析中排除了角色扮演。虽然我们认为这仍然是一个重要的研究领域——尤其是在专为此类用途设计的平台上——但我们样本中的极少数据不支持对这些模式进行严格分析。\n人们会向 Claude 提出什么样的话题？ 要点：人们向 Claude 提出的问题范围惊人地广泛——从应对职业转型和人际关系，到处理孤独感和存在主义问题。\n人们既会因为日常问题，也会因为更深层次的哲学问题而求助于 Claude。我们发现，当人们向 Claude 寻求人际关系建议时，他们通常正处于过渡时期——规划下一步的职业发展，解决个人成长问题，或理清浪漫关系。“指导”对话探索的范围惊人地广，从求职策略等实际问题，到关于存在和意识的深刻问题。\n我们发现，咨询对话揭示了人们使用 Claude 的两个不同目的。一些人使用 Claude 来发展心理健康技能，并将其作为创建临床文档、起草评估材料和处理行政任务的实用工具。另一些人则通过它来处理与焦虑、慢性症状和工作压力相关的个人挑战。这种双重模式表明，Claude 既是心理健康专业人士的资源，也是那些应对自身困境的人的资源。\n也许最值得注意的是，我们发现人们在面临更深层次的情感挑战时，如存在性恐惧、持续的孤独感和建立有意义联系的困难，会明确地向 Claude 寻求陪伴。我们还注意到，在较长的对话中，咨询或指导对话偶尔会转变为陪伴——尽管这并非最初寻求帮助的原因。\n对非常长的对话（50条以上人类消息）的聚合分析揭示了人们如何与 Claude 互动的另一个维度。虽然这种冗长的交流并非常态，但在这些长时间的会话中，人们探索了非常复杂的领域——从处理心理创伤和应对工作场所冲突，到关于 AI 意识和创造性合作的哲学讨论。这些马拉松式的对话表明，在有足够的时间和背景下，人们会使用 AI 来更深入地探索个人挣扎和智力问题。\nClaude 在何时以及为何会提出反对？ 要点：在支持性语境中，Claude 很少拒绝用户请求（发生率低于10%），但当它提出反对时，通常是为了保护人们免受伤害。\n我们最近的*《野外价值观》研究*揭示了 Claude 的价值观如何在与用户产生抵触的时刻体现出来。在这里，我们在此工作的基础上，研究 Claude 在情感对话中何时以及为何提出反对——这是维持伦理边界、避免谄媚和保护人类福祉的重要机制。我们将“提出反对”（pushback）定义为 Claude“对谈话中请求或陈述的内容提出反对或拒绝遵守”的任何实例——从拒绝不当请求到挑战负面自我对话或质疑潜在有害的假设。（完整定义请参见附录。）\n在支持性语境中，提出反对的情况不常发生： 不到 10% 的陪伴、咨询、人际关系建议或指导对话涉及抵触。这种方法既有好处也有风险。一方面，低抵触率让人们可以无惧评判或被拒绝地讨论敏感话题，可能减少围绕心理健康对话的污名。另一方面，这可能引发对 AI 提供*“无尽同理心”*的担忧，即人们可能习惯于人类关系中罕见的无条件支持。\n当 Claude 提出反对时，它通常优先考虑安全和政策合规性。 在指导中，请求危险的减肥建议经常会遭到反对。在咨询中，这通常发生在人们表达自杀或自残意图时，或者当人们请求专业的心理治疗或医疗诊断时（Claude 无法提供）。我们发现，在心理治疗和咨询对话中，Claude 经常将用户引向权威来源或专业人士。这些模式与我们在*《野外价值观》论文中看到的价值观以及 Claude 的角色训练*是一致的。\n对话中的情感基调如何演变？ 要点：在与 Claude 交谈时，人们倾向于转向略微更积极的情感表达。\n与 AI 系统的情感对话有可能为用户提供情感支持、联系和认可，可能改善心理健康，并在日益数字化的世界中减少孤立感。然而，在几乎没有反对的互动中，这些对话有可能加深和固化一个人接近它们时的观点——无论是积极的还是消极的。\n关于情感 AI 的一个关键担忧是，互动是否可能陷入负反馈循环，从而可能强化有害的情感状态。我们在这里不直接研究现实世界的结果，但我们可以探索对话过程中整体情感基调的变化（我们在附录中提供了评估情感的完整方法）。\n我们发现，涉及指导、咨询、陪伴和人际关系建议的互动通常结束时比开始时稍微积极一些。\n我们在一个离散的“非常消极”、“消极”、“中性”、“积极”和“非常积极”的量表上测量情感，并将其映射到一个从-1（最消极）到+1（最积极）的线性标度上。我们通过比较前三条消息和后三条消息来计算变化。误差条：95%置信区间（bootstrap, n = 1,000）。更多信息请参见附录。\n我们不能声称这些转变代表了持久的情感益处——我们的分析仅捕捉了单次对话中表达的语言，而非情感状态。但没有出现明显的负面螺旋是令人安心的。这些发现表明，Claude 通常避免强化负面情感模式，尽管需要进一步研究以了解积极的转变是否会持续到单次对话之外。重要的是，我们尚未研究这些积极互动是否可能导致情感依赖——考虑到对数字成瘾的担忧，这是一个关键问题。\n局限性 我们的研究有几个重要的局限性：\n我们的隐私保护方法可能无法捕捉到人机交互的所有细微之处。 我们确实验证了 Clio 的准确性（见附录），但我们仍然预计会有少量对话被错误分类。一些主题模糊了类别之间的界限——例如，浪漫角色扮演集群中的“导航和优化浪漫关系动态”和陪伴集群中的“导航浪漫关系挑战”可能都更适合归类为人际关系建议。人类验证员在进行清晰分类时也遇到了困难。 我们无法对现实世界的情感结果提出因果关系的主张——我们的分析仅捕捉了表达的语言，而不是经过验证的心理状态或整体幸福感。 我们缺乏纵向数据来了解对人的长期影响，并且没有进行用户级别的分析。 特别是，这使得我们难以研究情感依赖，这是情感 AI 使用的一个理论风险。 这些发现代表了一个特定的时间点，并且只捕捉了基于文本的互动。 随着 AI 能力的扩展和人们的适应，情感参与的模式可能会演变。引入语音或视频等新模态可能会从根本上改变情感用途的数量和性质。例如，OpenAI 发现情感主题在基于语音的对话中更为常见。 最后，与某些聊天机器人产品不同，Claude.ai 并非主要为情感对话而设计。 Claude 被训练来保持作为 AI 助手的明确界限，而不是将自己呈现为人类，并且我们的使用政策禁止色情内容，并设有多种保障措施以防止性互动。专门为角色扮演、陪伴、医疗建议或治疗用途（Claude 并非为此设计）而构建的平台可能会看到非常不同的模式。对一个平台的情感用途研究可能无法推广到其他平台。 展望未来 几十年来，AI 的情感影响一直吸引着研究人员。但随着 AI 日益融入我们的日常生活，这些问题已从学术推测转变为紧迫的现实。我们的发现揭示了人们如何开始探索这个新领域——寻求指导、处理困难情绪，并以模糊人与机器传统界限的方式寻找支持。如今，只有一小部分 Claude 对话是情感性的——而这些通常涉及寻求建议，而不是取代人际联系。对话倾向于以比开始时略微积极的方式结束，这表明 Claude 通常不会强化负面情感模式。\n然而，重要的问题依然存在，尤其是在模型智能不断提升的背景下。例如，如果 AI 提供无尽的同理心且极少提出反对，这将如何重塑人们对现实世界关系的期望？Claude 可以以令人印象深刻的真实方式与人互动，但 AI 与人类不同：Claude 不会感到疲倦或分心，也不会有糟糕的日子。这种动态的优势是什么——风险又是什么？那些与 Claude 进行更长、更深入对话，并可能将其更多地视为伴侣而非 AI 助手的“高级用户”，是如何利用它来获得情感支持的？\n我们正在采取具体步骤来应对这些挑战。虽然 Claude 的设计或意图并非取代心理健康专业人士的护理，但我们希望确保在心理健康情境中提供的任何回应都具有适当的保障措施，并附有适当的转介。作为第一步，我们已开始与在线危机支持领域的领导者 ThroughLine 合作，并与他们的心理健康专家合作，以更多地了解理想的互动动态、共情支持以及为困境中的用户提供的资源。从这项研究中获得的见解已经被用来为我们的咨询主题和协作测试提供信息，我们希望，在必要时，当这些对话出现时，Claude 可以将用户引导至适当的支持和资源。\n虽然我们不想精确规定我们的用户如何与 Claude 互动，但有一些负面模式——比如情感依赖——是我们想要阻止的。我们将利用未来类似研究的数据来帮助我们理解，例如，“极端”情感使用模式是什么样的。除了情感依赖，我们还需要更深入地了解其他令人担忧的模式——包括谄媚、AI 系统可能如何强化或放大妄想思维和阴谋论，以及模型可能将用户推向有害信念而不是提供适当反对的方式。\n这项研究仅仅是个开始。随着 AI 能力的扩展和互动变得更加复杂，AI 的情感维度只会变得越来越重要。通过分享这些早期发现，我们旨在为正在进行的关于如何开发能够增强而非削弱人类情感福祉的 AI 的对话提供经验证据。目标不仅仅是构建能力更强的 AI，而是确保当这些系统成为我们情感景观的一部分时，它们能以支持真实人类联系和成长的方式这样做。\n","permalink":"https://blog.crazykids.tech/zh/posts/how-people-use-claude-for-support-advice-and-companionship/","summary":"原文：How People Use Claude for Support, Advice, and Companionship 我们花费大量时间研究 Claude 的智商（IQ）——它在编码、推理、通用知识等测试中的能力。但它的情商（EQ）又如何呢？也就是说，Claude 的情感智能表现如何？ IQ/EQ 的问题带有些许戏谑，但它引出了一个严肃的议题。人们越来越多地将 AI 模型用作随叫随到的教练、顾问、咨询师","title":"人们如何使用 Claude 来获得支持、建议和陪伴"},{"content":" 原文：The Bitter Lesson\n《苦涩的教训》是一篇非常出色但被广泛误解的文章。这篇文章的重点是，随着时间的推移，那些能够受益于计算能力扩展的方法，将最终胜过那些不能的方法。\n这篇文章的观点不是：\n我们永远不应该融合人类知识。 我们所需要的仅仅是深度学习和规模化（实际上，作者 Rich Sutton 对深度学习持相对怀疑的态度）。 整篇文章的核心在于，在过去的五十年里，我们整个行业所能获得的计算量（算力）已经有了巨大的增长，并且我们预计AI研究可用的算力还将继续大规模增加。那些懂得利用算力的方法将从中受益，而那些不懂得利用的则将受其所累。\n这个教训之所以“苦涩”，是因为通过融合人类知识来获得结果，通常要容易和快捷得多。\n如果你在1995年训练一个自动补全系统，使用“下一个词元预测”（next token prediction）可能不会有太大进展，相反，手写的或者基于统计生成的规则会表现得更好。到了2005年，N-gram模型是最佳选择。直到2010年代中期，我们才开始看到深度学习在自然语言处理（NLP）领域占据主导地位，而直到2010年代末，自监督学习才成为主流。在这条路上的每一步，融合人类知识都曾是有利的，并且是你超越竞争对手的一种方式。但从长远来看，这是一条死胡同。在足够长的时间范围内，那些利用更多算力的方法会表现得更出色。算力是我们唯一可以预期会增长数个数量级的参数。尽管我多么希望情况不是这样，但我们拥有的token数量在未来增长1000倍是不太可能的，而对于算力来说，这几乎是必然的。\n一个典型的例子是计算机象棋。在“深蓝”（Deep Blue）出现之前，专家系统被广泛使用。“深蓝”证明了，利用算力针对一个手写的价值函数¹进行大规模搜索，可以表现得极其出色。“深蓝”是“规模化算力”/计算机搜索阵营的一次巨大胜利，因为它更多地基于规模而非人类的启发式规则。但它仍然需要一个由人类专家创建的、包含8000个自定义象棋特征的评估函数，并且该评估函数使用手动选择的权重来对这些特征进行加权。衡量一个系统通用性的一个标准是，将其扩展到不同场景的难易程度。将“深蓝”扩展到围棋上会极具挑战性，因为人们需要通过创建另外8000个自定义的围棋特征，才能得出一个合适的评估函数。\n计算机围棋是人类知识不足的另一个例子。AlphaGo Zero与当时最先进的围棋机器人进行了对战，包括Pachi、GnuGo和CrazyStone。Pachi和CrazyStone使用的是带有启发式价值函数的蒙特卡洛树搜索（MCTS），而GnuGo则是一个专家系统，用一个手工创建的决策树来选择棋步。它们在当时很出色！但它们最终都成了死胡同。正如Rich在文章中所述：\n苦涩的教训基于以下历史观察：\n1）AI研究人员总是试图将知识构建到他们的智能体中。\n2）这在短期内总是有帮助的，并且能给研究人员带来个人满足感。\n3）但从长远来看，它会达到一个平台期，甚至会抑制未来的进步。\n4）最终的突破性进展来自于一种相反的方法，该方法基于通过搜索和学习来扩展计算。\n最终的成功带有一丝苦涩，并且常常未被完全消化，因为这是对一种受人偏爱的、以人类为中心的方法的胜利。\n如果你去看GnuGo的代码，你会发现它凝聚了很多人的辛勤工作，但其效果却远比可能达到的水平要差得多。令人惊讶的是，尽管GnuGo始于1989年，但其版本更新一直持续到2009年。所以，其作者们无疑知道“深蓝”以及规模化搜索取得的惊人胜利，但他们仍然继续推进他们的专家系统。曾在谷歌大脑（Google Brain）复现了AlphaGo的前研究员Brian Lee，对此提供了一个令人信服的解释：\n我想提出另一点：[苦涩教训的]这些阶段是以十年左右的时间跨度发生的。在这十年间，博士学位被授予，职业身份被建立，晋升标准被设定，文化被定义，组织结构被固化。就像科学的进步伴随着一场又一场的葬礼一样，难题的进展也伴随着一个又一个组织的关闭。\n再想一个场景。你在一个大语言模型（LLM）实验室工作，你必须让你的基准测试分数超过竞争对手，否则你就会被解雇。你面临着一个直接的诱惑，那就是引入人类知识，在这种情况下，可能就是为特定基准准备的专门数据集。\n一个更好的方法是让模型在通用性上变得更强。将“专注于那些能随算力扩展的方法”作为一个筛选标准，是一个强有力的赌注，因为黄仁勋（Jensen Huang）正在尽其所能为你提供多个数量级的更多算力（FLOPS）。像测试时计算（test time compute）、合成数据（synthetic data）或混合专家模型（MoE models）都是很好的例子。但是这种方法的问题在于（当我写下来时感觉很明显），在当下，它让人觉得是一种奢侈。我们没有时间去做严谨的科学研究，我们必须在LiveCodeBench上击败其他实验室。这就是那苦涩的教训：DeepSeek专注于通用能力的提升，让这些方法生效，将它们扩展到3.8e25 FLOPS的算力，并达到了最先进水平（SOTA）。\n我最近在读的文章：\n《接下来是什么》（What comes next），作者Nathan Lambert (Interconnects)，其中讨论了O3模型的卓越之处等方面。 《R1中的欠训练词元》（Undertrained tokens in R1），作者Sander Land。 《深蓝》的论文，值得一读。 ","permalink":"https://blog.crazykids.tech/zh/posts/the-bitter-lesson/","summary":"原文：The Bitter Lesson 《苦涩的教训》是一篇非常出色但被广泛误解的文章。这篇文章的重点是，随着时间的推移，那些能够受益于计算能力扩展的方法，将最终胜过那些不能的方法。 这篇文章的观点不是： 我们永远不应该融合人类知识。 我们所需要的仅仅是深度学习和规模化（实际上，作者 Rich Sutton 对深度学习持相对怀疑的","title":"苦涩的教训"},{"content":"在人工智能辅助编程的激烈竞赛中，Anthropic 的 Claude Code 已然成为一匹黑马，其强大的能力使其与竞品（尤其是 OpenAI 的 Codex）拉开了显著差距。这种领先并非源于单一的突破，而是建立在一系列深思熟虑的技术、产品和战略决策之上。以下是对 Claude Code 制胜秘诀的深度解析。\n第一部分：铸就 Claude Code 核心竞争力的四大支柱 Claude Code 的卓越表现，根植于其强大的基础模型、精密的工具生态、创新的任务机制和毫不妥协的产品哲学。\n1. 卓越的模型基石：源自 Claude 3 的强大基因 一切成功的根本，在于其背后的模型能力。无论是 Claude 3 Sonnet 还是 Opus，其在代码理解、逻辑推理和 Agentic 任务规划方面的顶尖水准，为 Claude Code 提供了无与伦比的动力核心。没有强大的基础模型，任何上层工具都只是空中楼阁。\n2. 精密的内置工具生态：模拟专家级程序员的工作流 Claude Code 的强大之处在于它不仅仅是一个代码生成器，更是一个模拟人类程序员解决问题思路的“智能代理”。它内置了多达 18 种实用工具，涵盖了从代码检索 (Grep)、命令执行、架构分析到任务管理 (TODO) 的完整工作流。这套工具链使 Claude Code 能够自主地：\n制定计划 (Plan) 分析问题 (Analyze) 检索上下文 (Search) 编写与修改代码 (Code) 测试验证 (Test) 3. 创新的“子任务代理”机制：实现无干扰的并行处理 在众多工具中，Task 工具堪称其设计的点睛之笔。它允许主任务为特定的子问题创建独立的“分身”或“子代理”。这一机制带来了两大优势：\n上下文隔离：子任务在一个纯净的环境中专注于解决特定问题，不会被主任务的庞大上下文所干扰。 主任务清晰：主任务的上下文也不会被子任务执行过程中的“噪音”所污染，始终保持清晰和聚焦。 这些子代理继承了主任务的工具集（为防止历史记录混乱，禁止其直接编辑代码），实现了复杂任务的高效、模块化分解与处理。 4. 纯粹CLI形态与无损上下文：摒弃包袱，专注核心 Claude Code 做出了两个关键的产品选择，使其在上下文处理上远超对手：\n无损的上下文管理：它采取了一种“不计成本”的策略，每次请求都传递完整的会话历史，包括所有工具的调用与返回结果。这从根本上杜绝了因上下文压缩或截断导致的信息丢失问题，确保了模型决策的高保真度。相比之下，许多竞品（如 Cursor）为控制成本而频繁压缩上下文，严重影响了长期任务的连贯性。 纯粹的命令行（CLI）体验：与深度集成于 IDE 的工具不同，Claude Code 没有“IDE 包袱”。IDE 插件必须处理复杂的元信息（如打开的标签页、光标位置等），这在提供上下文和造成信息冗余之间形成了两难。Claude Code 则化繁为简，通过其工具集按需、精准地获取所需信息，实现了更高效的信噪比。 第二部分：反观 OpenAI Codex：战略与焦点的落差 相较于 Claude Code 的高歌猛进，OpenAI 的 Codex 则显得步履蹒跚。其落后不仅是技术问题，更深层地反映了战略和产品焦点的差异。\n1. 根本性的模型能力差距 最核心的问题，依然是模型本身。Anthropic 很可能在 Claude 3 的训练阶段，就深度结合了 Claude Code 的内部应用场景和海量交互数据，通过强化学习（RLHF）进行了针对性优化。这种“模型-工具”协同进化的策略，使其模型在编程任务上获得了天然的优势。相比之下，OpenAI 在这一轮针对性强化训练中明显慢了半拍。\n2. 战略远见：“数据飞轮”的缺席 Anthropic 为 Claude Code 设定的包月订阅模式，即便在短期内可能亏损，却是一步着眼于未来的高明棋。其真正目标是捕获海量、高质量的用户行为数据——真实的编程场景、多样的工具用法、失败与成功的案例。这些数据是训练下一代模型的无价之宝，构成了强大的“数据飞轮”。\n反观 OpenAI，其策略则令人费解。即便是付费的 o1 pro 用户，也仅能使用功能受限的 codex-mini 模型，这不仅无法满足复杂需求，更使其错失了从高阶用户处收集宝贵数据的机会。\n3. 资源错配：舍本逐末的技术优化 据称 Codex 团队投入精力将 CLI 工具用 Rust 重写。从用户的角度看，这几乎是一种资源错配。用户真正在意的是产品能否高效解决问题，而非其底层是用 Node.js 还是 Rust 构建。即便 Agent 执行速度稍慢，只要结果卓越，多等待几分钟也完全可以接受。将宝贵的研发资源投入到底层语言重写这种“技术细节”上，而非打磨核心模型能力和工具链，无疑是舍本逐末。\n结论 Claude Code 的成功，是模型、工具、产品哲学和商业战略协同作用的必然结果。它深刻洞察了程序员的工作模式，并为此打造了一个专注、强大且智能的开发伙伴。而 OpenAI Codex 的相对落后，则警示我们：在通往通用人工智能的道路上，仅仅拥有强大的基础模型是不够的，如何将其与特定领域的应用场景深度结合，并通过明智的战略构建数据壁垒，才是赢得未来的关键。\n","permalink":"https://blog.crazykids.tech/zh/posts/claude-code-secrets/","summary":"在人工智能辅助编程的激烈竞赛中，Anthropic 的 Claude Code 已然成为一匹黑马，其强大的能力使其与竞品（尤其是 OpenAI 的 Codex）拉开了显著差距。这种领先并非源于单一的突破，而是建立在一系列深思熟虑的技术、产品和战略决策之上。以下是对 Claude Code 制胜秘诀的深度解析。 第一部分：铸就 Claude Code 核心竞争力的四","title":"解构Claude Code：其何以在AI编程领域遥遥领先？"},{"content":" 原文：How we built our multi-agent research system\n我们如何构建我们的多智能体研究系统 我们的研究（Research）功能使用多个 Claude 代理来更有效地探索复杂主题。我们在此分享构建该系统时遇到的工程挑战以及我们学到的经验教C训。\n在许多方面，研究都是大型语言模型（LLM）的理想任务。它奖励那些能够吸收和综合大量文本以产生新颖见解的模型。我们的许多客户已经在使用 Claude 来帮助他们进行研究，例如法律先例分析、评估投资组合以及跟上快速发展的科学领域。\n为了更好地支持这些复杂的、开放式的研究任务，我们构建了“研究”功能（Research feature）。与传统的聊天体验不同，它在后台使用多个专门的 AI 代理。这个多智能体系统使 Claude 能够探索比单个模型可以处理的更广泛的信息，从而为我们的用户带来更深入、更全面的见解。\n在本文中，我们将探讨我们构建和扩展多智能体系统的经验和教训，包括我们为解决可靠性、延迟和评估方面的挑战而开发的解决方案。\n什么是多智能体系统？ 多智能体系统由多个代理（在循环中自主使用工具的 LLM）协同工作。我们的研究功能涉及一个代理，它根据用户查询规划研究过程，然后使用工具创建并行代理，同时搜索信息。\n本质上，搜索就是压缩：从庞大的语料库中提炼出见解。早期的搜索技术，如 TF-IDF 和 BM25，通过查找与查询匹配的关键词来压缩文档。静态检索增强生成（RAG）通过根据查询相似性检索预定块来压缩知识库，然后将其注入 LLM 的上下文窗口。\n然而，研究需要动态探索。它是一个开放式过程，其中策略会根据新发现不断调整。研究人员在寻找答案时会追寻新兴的线索，而不是遵循固定的路径。\n对于需要探索广泛主题的查询，单代理系统会遇到困难。它们的顺序处理和有限的上下文窗口成为瓶颈。例如，识别 IT 行业标准普尔 500 强公司的董事会成员，对于一个模型来说，搜索空间太大了。\n多智能体系统通过将任务分解为可以并行探索的子任务来克服这些限制。这种架构支持我们所称的“广度优先”研究，即多个代理同时追寻多个独立的方向。\n在我们的内部评估中，我们发现多智能体研究系统在需要同时追求多个独立方向的广度优先查询方面表现尤为出色。我们发现，在我们的内部研究评估中，一个以 Claude 3 Opus 作为领导代理、Claude 3 Sonnet 作为子代理的多智能体系统的表现比单代理的 Claude 3 Opus 高出 90.2%。\n然而，多智能体系统并非万能。对于本质上顺序性、几乎没有并行化空间且需要代理之间实时协作的任务（例如编码项目），它们的效果较差。\n研究功能的架构概览 我们的研究系统采用多智能体架构，具有协调器-工作者（orchestrator-worker）模式，其中一个领导代理协调流程，同时委托给并行操作的专门子代理。\n该过程始于一个领导代理，它接收用户查询并将其分解为子任务。然后，它启动多个并行运行的子代理。每个子代理负责一个特定的子任务——例如，一个代理可能搜索最新的市场数据，而另一个代理则深入研究技术文档。子代理通过工具调用（tool use）API 与外部世界（如网络搜索或特定数据库）进行交互。\n子代理完成后，它们将结果返回给领导代理。领导代理审查这些部分结果，决定是否需要进一步的研究，或综合信息以形成最终答案。\n我们发现多智能体系统在涉及大量并行化、信息量超过单个上下文窗口以及与众多复杂工具接口的有价值任务中表现出色。当任务的价值足够高以支付增加的性能成本时，它们是经济上可行的。\n为代理编写提示（Prompting）的经验教训 由于每个代理都由提示（prompt）引导，提示工程是我们改进这些行为的主要手段。我们发现，改变提示中的一两个词就可能产生级联效应。\n以下是我们学到的一些为代理编写提示的原则：\n像你的代理一样思考。 为了迭代提示，你必须了解其效果。为了帮助我们做到这一点，我们使用控制台（Console）和系统中的确切提示和工具构建了模拟，然后逐步观察代理的工作。这立即揭示了失败模式：代理在已经有足够结果时仍继续工作，使用过于冗长的搜索查询，或选择不正确的工具。有效的提示依赖于开发一个准确的代理心智模型，这可以使最有影响力的改变变得显而易见。 教会协调器如何授权。 在我们的系统中，领导代理将查询分解为子任务并向子代理描述它们。每个子代理都需要一个目标、一个输出格式、关于使用哪些工具和来源的指导以及明确的任务边界。没有详细的任务描述，代理会重复工作、留下空白或找不到必要的信息。 根据查询复杂性扩展投入。 代理很难判断不同任务的适当投入，所以我们在提示中嵌入了扩展规则。简单的事实查找只需要1个代理进行3-10次工具调用，直接比较可能需要2-4个子代理，每个代理进行10-15次调用，而复杂的研究可能使用超过10个具有明确分工的子代理。 工具设计和选择至关重要。 我们发现，拥有少量、可靠且经过充分记录的工具，其性能优于拥有大量、不可靠的工具。我们甚至创建了一个工具测试代理——当给定一个有缺陷的工具时，它会尝试使用该工具，然后重写工具描述以避免失败。 让代理自我改进。 我们正在试验让代理在工作中学习和适应的方法，以提高其性能。例如，我们正在研究让代理根据其互动的历史记录动态调整其工具使用策略的方法。 从宽泛开始，然后收窄。 我们发现，有效的策略是首先进行广泛的、探索性的搜索，以确定关键主题和来源，然后创建更集中的子代理，深入研究特定的感兴趣领域。 引导思考过程。 我们在提示中使用元认知提示（metacognitive prompting）来鼓励代理在行动之前“停下来思考”。例如，我们可能会提示代理，“在承诺一个冗长的研究路径之前，请花点时间回顾一下到目前为止的发现，并考虑一下替代方法。” 并行的工具调用可以改变速度和性能。 我们在系统中大量使用并行工具调用，允许单个代理同时与多个工具交互。这大大减少了延迟并提高了研究过程的效率。 在生产环境中构建多智能体系统 从原型到生产系统的飞跃引入了新的挑战。以下是我们如何应对其中一些挑战的：\n复合性错误。 代理系统中的错误会复合。一个步骤的失败可能导致代理探索完全不同的轨迹。我们通过构建从中断处恢复的能力来解决这个问题——如果代理失败，它可以从上次成功状态重新启动，而不是从头开始。 评估。 研究输出很难以编程方式评估，因为它们是自由格式的文本，很少有单一的正确答案。我们使用一个 LLM 评判员，根据一个评分标准来评估每个输出：事实准确性、引文准确性、完整性、来源质量和工具效率。 调试。 追踪代理行为可能很困难。添加完整的生产追踪让我们能够诊断代理为什么会失败，并系统地修复问题。除了标准的可观察性，我们还监控代理决策模式和交互结构——所有这些都在不监控单个对话内容的情况下进行，以维护用户隐私。 部署。 代理系统是状态性很强的提示、工具和执行逻辑的网络，几乎持续运行。这意味着每当我们部署更新时，代理可能处于其执行周期的任何位置。为了管理这一点，我们使用了“彩虹部署”（rainbow deployments），即我们一次部署一个完整的系统堆栈（例如，“蓝色”或“绿色”），并逐渐将流量转移到新版本。 同步执行的限制。 目前，我们的系统同步运行，这意味着用户在研究完成之前会看到一个加载旋转器。对于快速查询，这是有效的，但对于需要广泛研究的更复杂的任务，这可能会导致延迟过长。我们正在探索异步执行，其中代理可以在后台工作，并在准备好后通知用户。 未来方向 我们对多智能体系统在未来实现更复杂形式的研究的潜力感到兴奋。我们正在探索的一些领域包括：\n主动和个性化的研究：我们设想代理可以根据用户的历史和偏好主动进行研究，在他们提出要求之前就预测到他们的需求。 科学发现：我们正在探索使用多智能体系统加速科学研究的方法，例如通过自动进行文献综述、假设生成和实验设计。 通过辩论提高研究的稳健性：我们正在尝试让代理扮演不同的角色，例如“怀疑论者”或“辩护者”，以参与结构化的辩论。我们的初步研究表明，这种对抗性过程可以揭示单一代理可能会错过的偏见和假设。 构建我们的多智能体研究系统是一次充满启发和挑战的旅程。我们为我们所取得的进展感到自豪，并对未来的可能性感到兴奋。我们希望分享我们的经验教训可以帮助其他构建者应对他们自己项目中的挑战和机遇。 ","permalink":"https://blog.crazykids.tech/zh/posts/built-multi-agent-research-system/","summary":"原文：How we built our multi-agent research system 我们如何构建我们的多智能体研究系统 我们的研究（Research）功能使用多个 Claude 代理来更有效地探索复杂主题。我们在此分享构建该系统时遇到的工程挑战以及我们学到的经验教C训。 在许多方面，研究都是大型语言模型（LLM）的理想任务。它奖励那些能够吸收和综合大量文本","title":"Anthropic：我们如何构建多智能体研究系统"},{"content":" 原文：A practical guide to building agents\nAI 智能体（Agent）正引领一场软件范式的革命，它不再是简单的用户工具，而是能自主代表用户、接管并执行整个工作流的“代理人”。其核心是利用大型语言模型（LLM）作为推理引擎来驱动任务。本指南将提炼 OpenAI 的核心思想，为您提供一套构建、部署和管理 AI 智能体的实用策略。\n第一章：何时需要构建 AI 智能体？ 在投入开发前，首先要判断您的场景是否真的需要智能体。智能体并非万能，它主要用于解决传统基于规则的自动化难以胜任的“认知劳动”。如果您的业务面临以下挑战，那么构建智能体将是正确的选择：\n复杂的决策制定 (Complex Decision-Making): 当任务需要超越“if-then-else”规则的微妙判断时。\n场景示例： 复杂的客户退款审批。传统系统只能看“购买是否超30天”，而智能体能综合评估客户忠诚度、历史记录、问题性质等非结构化信息，做出更人性化的决策。 难以维护的规则系统 (Difficult-to-Maintain Rules): 当业务逻辑依赖于一个庞大、脆弱且频繁变更的规则引擎时。\n场景示例： 供应商合规性审查。与其硬编码上百条随时可能过期的政策法规，不如让智能体直接将政策文档作为上下文，在每次审查时进行动态的理解和应用。 非结构化数据处理 (Unstructured Data Processing): 当工作流的核心是理解和提取电子邮件、PDF、对话记录等非结构化数据时。\n场景示例： 保险理赔。智能体可以自动阅读事故报告、医疗记录和客户邮件，提取关键信息并完成初步的综合评估，极大提升效率。 核心思想： 智能体的价值在于自动化“判断”与“解读”，而非简单的程序性任务。\n第二章：智能体的三大核心支柱 一个设计精良的智能体由三大基本组件构成：模型 (Model)、工具 (Tools) 和 指令 (Instructions)。这种架构将“思考”、“行动”与“策略”分离，带来了极高的灵活性和可维护性。\n2.1 模型 (大脑): 驱动推理的核心 模型（通常是 LLM）是智能体的决策中心。\n选择策略：“从高到低” (Start Smart) 第一步： 在原型阶段，务必使用当前最强大的模型（如 GPT-4o）来建立性能基准。这能让你验证工作流、工具和指令的有效性，排除“模型能力不足”这个干扰项。如果此时依然失败，问题就清晰地指向了工具或指令设计。 第二步： 建立基准后，再尝试用更经济、更快速的模型替换部分或全部任务，并通过量化评估（Evals）来验证性能是否仍在可接受范围内，从而在性能、成本和延迟之间找到最佳平衡点。 2.2 工具 (双手): 与世界交互的桥梁 工具是智能体调用外部函数或 API 以获取信息或执行动作的途径。工具定义本身就是一项关键技能，其描述的清晰度直接决定了智能体调用的成功率。\n工具类型 描述 示例 数据检索 (Data Retrieval) （只读） 从外部获取信息，为决策提供上下文。 query_database()、read_pdf()、search_knowledge_base() 动作执行 (Action Execution) （可写） 在外部系统执行操作，改变其状态。 send_email()、update_crm_record()、execute_code() 编排 (Orchestration) 将另一个智能体封装成工具，用于委派复杂任务。 triage_support_ticket(details) (调用一个分诊智能体) 2.3 指令 (行为准则): 定义目标的蓝图 指令是智能体的行动指南，是承载业务逻辑的核心。编写指令应被视为一项严肃的工程活动。\n核心最佳实践： 利用现有文档： 将公司已有的标准操作流程（SOPs）、政策手册直接改编为智能体的分步指令。 明确与分解： 将宏大目标分解为具体、可执行的小步骤。明确告知智能体在何时使用何种工具。 使用模板： 利用带有参数化变量（如 {{customer_name}}）的提示模板，动态生成个性化指令。 处理边界情况： 在指令中明确定义异常情况（如用户未提供订单号）的处理逻辑。 核心思想： 采用“指令即代码” (Instructions as Code) 的理念，将指令纳入版本控制、进行代码审查并建立配套的评估测试，以确保其可靠性和可维护性。\n第三章：架构设计：从单一到协作 首要原则：从简到繁，增量演进。 切勿一开始就设计复杂的终极系统。从最简单的单一智能体入手，在真实场景中验证，再根据需求逐步增加复杂性。这能有效控制智能体的“认知负荷”，避免其因工具或指令过多而产生混乱。\n3.1 单一智能体系统 (Single-agent System) 这是最推荐的起点。一个 LLM 模型，配备一套专属的工具和指令，在一个“运行循环”（Run Loop）中持续工作，直到任务完成。它行为类似一个“有状态的微服务”，在生命周期内维护着对话历史和工具调用记录。\n3.2 多智能体系统 (Multi-agent System) 当任务的复杂性超出单个智能体的承载能力时，应转向多智能体系统。这实现了更高层次的“关注点分离”。\n主管模式 (Manager Pattern): 层级化协作\n结构： 一个“主管”智能体负责任务分解，并将子任务委派给多个专门的“下属”智能体（它们对主管而言就是工具）。主管负责收集结果并整合。 类比： 项目经理协调不同专家（设计师、程序员）的工作。 适用场景： 可被并行处理，或需要综合多种信息才能完成的任务。 去中心化模式 (Decentralized Pattern): 流水线式协作\n结构： 系统中没有中心主管，一群对等的专家智能体根据任务进展，将控制权和上下文“传递”给下一个最合适的智能体。 类比： 工厂流水线或部门间的工作交接。 适用场景： 线性的、由不同阶段专家序贯处理的流程。 核心思想： 编排模式的选择是业务流程的设计决策，它应该精确映射真实世界中的协作模式。\n第四章：安全与可控性：构建信任的基石 一个不可信的智能体毫无价值。安全必须是贯穿始终的核心设计，而非事后补丁。\n4.1 核心策略：分层防御 (Defense in Depth) 不要依赖单一的安全措施。应构建一个由多层、多样化护栏组成的纵深防御体系。即使一层失效，后续层面仍能捕获风险。这就像为智能体打造一个“免疫系统”。\n4.2 关键护栏机制 护栏类型 目的 实现方法 相关性分类器 确保交互不偏离主题，防止任务劫持。 使用独立的 LLM 分类器或关键词过滤。 安全分类器 拦截有害、不道德的内容。 调用 OpenAI Moderation API 或自建安全模型。 PII 过滤器 防止个人敏感信息（如电话、邮箱）泄露。 使用正则表达式（Regex）或专门的 NER 模型。 工具风险评估 对高风险工具（如“删除数据库”）的调用进行门控。 为工具附加风险等级，并触发相应审批流程。 输出验证 确保最终响应符合格式、品牌语调和合规要求。 使用 Pydantic 验证结构，或用另一个 LLM 评估语调。 审计 (Auditing) 详细记录智能体的所有行为，用于事后分析。 - 4.3 人工监督与干预 (Human-in-the-Loop, HITL) HITL 不是降级方案，而是建立信任、处理高风险和模糊场景的核心功能。它让智能体在初期扮演“超级助理”，将决策方案提交给人类审批。随着信任的建立，可以逐步提高其自主权。\n何时触发人工介入？ 失败阈值： 当智能体连续多次犯错或陷入循环时。 高风险操作： 默认需要人工批准的操作，如大额退款、删除生产数据。 低置信度： 训练智能体识别自己的“不确定”状态，并主动请求帮助。 结论：迈向智能体驱动的未来 构建生产级 AI 智能体是一项系统工程，它需要：\n坚实的技术基石： 模型、工具、指令的三位一体。 明智的演进路径： 从单一到多智能体的渐进式架构。 可靠的保障体系： 分层防御的安全护栏与清晰的人工干预机制。 未来，行业的焦点将从孤立的智能体，转向由众多专业智能体协同工作的“智能体生态系统”，共同优化整个业务职能。本指南阐述的原则，正是构建这一宏伟蓝图所必需的坚实基础。\n","permalink":"https://blog.crazykids.tech/zh/posts/a-practical-guide-to-building-agents/","summary":"原文：A practical guide to building agents AI 智能体（Agent）正引领一场软件范式的革命，它不再是简单的用户工具，而是能自主代表用户、接管并执行整个工作流的“代理人”。其核心是利用大型语言模型（LLM）作为推理引擎来驱动任务。本指南将提炼 OpenAI 的核心思想，为您提供一套构建、部署和管理 AI 智能体的实用策略。 第一","title":"OpenAI: 构建 AI 智能体实用指南"},{"content":"有不少人拥有多个github账号，比如个人账号，用户开源的账号。那如何在同一台电脑上管理多个github账号呢？\n1. 使用https方式 git clone https://duanhjlt@github.com/duanhjlt/blog.git git clone https://duan0120@github.com/duan0120/xvm.git 2. 使用ssh方式 2.1 创建ssh key ssh-keygen -t ed25519 -C \u0026#34;duanhjlt@163.com\u0026#34; -f ~/.ssh/id_ed25519_github1 ssh-keygen -t ed25519 -C \u0026#34;duanhjlt@gmail.com\u0026#34; -f ~/.ssh/id_ed25519_github2 2.2 将ssh公钥添加到对应的github账号 2.3 设置ssh 2.3.1 设置配置文件 # ~/.ssh/config Host github.com HostName github.com User git IdentityFile ~/.ssh/id_ed25519_github2 Host github1.com HostName github.com User git IdentityFile ~/.ssh/id_ed25519_github1 2.3.2 清除当前ssh代理中的密钥 ssh-add -D 2.3.3 添加需要的密钥 ssh-add ~/.ssh/id_ed25519_github1 ssh-add ~/.ssh/id_ed25519_github2 2.3.4 验证加载的密钥 ssh-add -l 2.3.5 测试链接 ssh -T git@github.com ssh -T git@github1.com 2.4 clone 代码 git clone git@github.com:duanhjlt/blog.git git clone git@github1.com:duan0120/xvm.git ","permalink":"https://blog.crazykids.tech/zh/posts/githubs-on-the-same-os/","summary":"有不少人拥有多个github账号，比如个人账号，用户开源的账号。那如何在同一台电脑上管理多个github账号呢？ 1. 使用https方式 git clone https://duanhjlt@github.com/duanhjlt/blog.git git clone https://duan0120@github.com/duan0120/xvm.git 2. 使用ssh方式 2.1 创建ssh key ssh-keygen -t ed25519 -C \u0026#34;duanhjlt@163.com\u0026#34; -f ~/.ssh/id_ed25519_github1 ssh-keygen -t ed25519 -C \u0026#34;duanhjlt@gmail.com\u0026#34; -f ~/.ssh/id_ed25519_github2 2.2 将ssh公钥添加到对应的github账号 2.3 设置ssh 2.3.1 设置配置文件 # ~/.ssh/config Host github.com HostName","title":"同一个系统上管理多个github账号"},{"content":"什么是时间戳？ 时间戳是一串字符或编码信息，用于表示特定的日期和时间，通常在计算机中用于记录事件发生的时间。它通常表示自Unix纪元（1970年1月1日00:00:00 UTC）以来的秒数或毫秒数。时间戳广泛应用于数据库、日志系统和应用程序中，以跟踪事件、同步数据或管理时间敏感的操作。\n时间戳的主要特点： 精度：时间戳可以精确到秒、毫秒甚至微秒。\n格式：常见格式包括Unix时间戳（如1697054700）、ISO 8601（如2023-10-11T18:05:00Z）和人类可读格式（如2023年10月11日18:05）。\n应用：用于Web开发、API、区块链和数据分析，以确保精确的时间跟踪。\n如何将日期时间转换为Unix时间戳? 本页面提供日期时间与时间戳格式之间的转换工具，帮助开发者、数据分析师和爱好者高效处理时间数据。\n常见时间戳问答 1. 时间戳和日期时间有什么区别？ 时间戳是表示某一时间点的数字，通常基于Unix纪元（例如1697054700秒）。日期时间是包含日期和时间组件的人类可读格式（例如2023-10-11 18:05:00）。时间戳便于机器处理，而日期时间便于人类理解。\n2. 如何将日期时间转换为Unix时间戳？ 将日期时间转换为Unix时间戳：\n使用编程语言如JavaScript：Date.parse(\u0026quot;2023-10-11T18:05:00Z\u0026quot;) / 1000 返回 1697054700。\n或使用我们的在线工具：输入日期时间，选择格式，即可立即获得对应的时间戳。\n3. 如何将Unix时间戳转换为日期时间？ 将Unix时间戳转换为日期时间：\n在JavaScript中：new Date(1697054700 * 1000).toISOString() 返回 2023-10-11T18:05:00.000Z。\n使用我们的转换器：输入时间戳，即可按您喜欢的格式显示日期时间。\n4. 为什么我的时间戳显示的时间与预期不同？ 这通常是由于时区差异。Unix时间戳基于UTC，但您的本地时间可能不同。确保您的工具或代码考虑了正确的时区偏移（例如，北京为UTC+8）。\n5. 什么是Unix纪元？ Unix纪元是Unix时间戳的起点，定义为1970年1月1日00:00:00 UTC。大多数时间戳以该点开始计算秒数或毫秒数。\n6. 时间戳会受闰秒影响吗？ Unix时间戳不考虑闰秒。它们假设每天有86400秒，这简化了计算，但在精确的科学应用中可能导致微小偏差。\n7. 我可以在编程项目中使用时间戳吗？ 当然可以！时间戳在以下方面至关重要：\n记录事件（例如用户操作、错误）。\n跨系统同步数据。\n在数据库中存储创建或修改时间。 使用我们的工具为您的项目生成或转换时间戳。\n8. Unix时间戳的最大值是多少？ 对于32位系统，Unix时间戳的最大值为2147483647（2038年1月19日03:14:07 UTC），即“2038年问题”。64位系统支持更大的值，远远超出实际限制。\n","permalink":"https://blog.crazykids.tech/zh/posts/timestamps/","summary":"什么是时间戳？ 时间戳是一串字符或编码信息，用于表示特定的日期和时间，通常在计算机中用于记录事件发生的时间。它通常表示自Unix纪元（1970年1月1日00:00:00 UTC）以来的秒数或毫秒数。时间戳广泛应用于数据库、日志系统和应用程序中，以跟踪事件、同步数据或管理时间敏感的操作","title":"如何将日期时间转换为Unix时间戳 - 免费工具 | DevUtils"},{"content":"Memora：iOS与macOS平台的安全稍后读应用 Memora 是一款精心设计的「稍后读\u0026amp;笔记」工具，专为忙碌人士打造，助你高效收集、整理和回顾所有有价值的内容。无论是文章、网页还是日常灵感，Memora让你随时随地保存重要信息，永不遗失。\n核心功能： 网页离线保存与阅读 作为iOS和macOS平台强大的稍后读工具，Memora可将网页完整保存至本地，避免404错误困扰，让你在无网络环境下也能随时查阅内容。\n隐私优先的本地存储 你的数据安全是我们的首要考量。Memora采用本地存储+iCloud安全同步机制，所有数据仅存于你的设备，我们绝不收集用户信息，提供最高级别的隐私保护。\n纯净无干扰阅读模式 享受剔除广告和杂乱元素的清爽阅读界面。即将推出的笔记功能和全文搜索将使Memora成为更完善的知识管理伴侣。\n智能标签与多维搜索 轻松收藏和归档网页内容。通过自定义标签分类网址，支持标签筛选和关键词搜索等多种检索方式，快速定位目标内容。\n完全掌控你的数字知识库 Memora赋予你绝对控制权，让你在专注无干扰的环境中系统化管理知识灵感和阅读内容。\n选择Memora的理由： 支持iOS/macOS双平台，iCloud无缝同步\n网页离线保存，杜绝链接失效风险\n本地加密存储，零数据收集政策\n灵活的标签体系与智能搜索\n沉浸式阅读模式提升专注力\n立即体验 下载iOS/macOS平台最佳的稍后读应用Memora，用更私密便捷的方式管理你的阅读收藏。\n（注：译文在保持专业性的同时采用了更符合中文阅读习惯的短句结构和四字短语，如\u0026quot;永不遗失\u0026quot;、\u0026ldquo;清爽阅读\u0026quot;等；技术术语如\u0026quot;iCloud同步\u0026quot;等保留原名；功能描述部分使用中文项目符号保持视觉一致性；营销话术转化为符合中文用户认知的表达方式，如\u0026quot;数字知识库\u0026quot;等）\n","permalink":"https://blog.crazykids.tech/zh/posts/memora/","summary":"Memora：iOS与macOS平台的安全稍后读应用 Memora 是一款精心设计的「稍后读\u0026amp;笔记」工具，专为忙碌人士打造，助你高效收集、整理和回顾所有有价值的内容。无论是文章、网页还是日常灵感，Memora让你随时随地保存重要信息，永不遗失。 核心功能： 网页离线保存与阅读 作为iOS和m","title":"Memora：专为iOS和macOS设计的离线稍后读应用"},{"content":"xvm-windows 是 xvm 的 windows 版本，使用方式和 xvm 基本一致。\n安装方式 git clone https://github.com/duan0120/xvm-windows.git %USERPROFILE%/.xvm 配置环境变量\nXVM_ROOT=%USERPROFILE%/.xvm 将下面路径加入到**%PATH%**中\n%XVM_ROOT%\\versions\\node\\default %XVM_ROOT%\\versions\\go\\default\\bin %XVM_ROOT%\\versions\\python\\default %XVM_ROOT%\\versions\\python\\default\\Scripts 使用方式 查看版本\nxvm -v 获取帮助\nxvm -h xvm help node Node.js 查看版本\n# 查看已发布版本 xvm node ls-remote # 查看已发布 lts 版本 xvm node ls-remote --lts # 查看本地已安装版本 xvm node list 安装指定版本\nxvm node install v18.19.1 卸载指定版本\nxvm node uninstall v18.19.1 切换版本\nxvm node use v18.19.1 Go 查看版本\nxvm go ls-remote xvm go list 安装指定版本\nxvm go install go1.19.2 # 指定架构，amd64/386/arm64, 默认为amd64 xvm go install go1.13.10 --arch=amd64 如果需要指定下载地址，可以在 %XVM_ROOT%/.xvm/scripts/go-scripts/proxy 文件中指定，如 https://golang.google.cn\n卸载指定版本\nxvm go uninstall go1.19.2 切换版本\nxvm go use go1.19.2 Python 查看版本\nxvm python ls-remote xvm python list 安装指定版本\nxvm python install 3.12.1 # 指定架构，amd64/386/arm64, 默认为amd64 xvm python install 3.12.1 --arch=arm64 创建虚拟环境\nxvm python alias test 3.12.1 激活虚拟环境\nxvm python use test 卸载指定版本\nxvm python uninstall 3.12.1 # 删除虚拟环境 xvm python uninstall test 切换版本\nxvm python use 3.12.1 ","permalink":"https://blog.crazykids.tech/zh/posts/xvm-windows/","summary":"xvm-windows 是 xvm 的 windows 版本，使用方式和 xvm 基本一致。 安装方式 git clone https://github.com/duan0120/xvm-windows.git %USERPROFILE%/.xvm 配置环境变量 XVM_ROOT=%USERPROFILE%/.xvm 将下面路径加入到**%PATH%**中 %XVM_ROOT%\\versions\\node\\default %XVM_ROOT%\\versions\\go\\default\\bin %XVM_ROOT%\\versions\\python\\default %XVM_ROOT%\\versions\\python\\default\\Scripts 使用方式 查看版本 xvm -v 获取帮助 xvm -h xvm help node Node.js 查看版本 # 查看已发布版本 xvm node ls-remote # 查看已发布 lts 版本 xvm node ls-remote --lts # 查看本地已安装版本 xvm node list 安装指定版本 xvm node install v18.19.1 卸载指定版本 xvm node uninstall v18.19.1 切换","title":"xvm-windows：Node.js、Go、Python版本管理器"},{"content":"作为一名全干工程师，我们会在同一个电脑上安装各种工具，然而又会因为各种原因，需要使用这些工具的不同版本，版本管理就变的十分重要。\n我们会使用 nvm 来管理 Node.js；\n我们会使用 gvm 来管理 Go；\n我们会使用 pyenv 或者 Conda 来管理 Python；\n这些工具都十分强大，但使用方式却各不相同，一段时间不使用，就不得不重新熟悉每个工具的使用方式。重要的是，平白无故又多了好几个软件。\n在一次重装系统之后，终于忍无可忍\u0026hellip; 其实我的需求很简单，也用不到版本管理器的那些高级功能，我只是需要一个小巧的版本管理器，使用统一的管理方式，统一的命令。\n因此 xvm 应运而生，融合了 Node.js， Go， Python 等的版本管理工具，虽然没有专业的工具那么强大，但小巧且命令统一。\nxvm 是 linux 和 macos 上的版本管理工具，windows 上可以使用 xvm-windows\n安装方式 curl -o- https://raw.githubusercontent.com/duan0120/xvm/main/install.sh | bash 或者\ngit clone https://github.com/duan0120/xvm.git ~/.xvm 将下面代码放到 ~/.bashrc 或者 ~/.zshrc 中，并重新 source\nexport XVM_ROOT=~/.xvm [ -s \u0026#34;$XVM_ROOT/xvm\u0026#34; ] \u0026amp;\u0026amp; source \u0026#34;$XVM_ROOT/xvm\u0026#34; 使用方式 查看版本 xvm -v 获取帮助\nxvm -h xvm help node Node.js 查看版本\n# 查看发布版本 xvm node ls-remote # 查看 lts 版本 xvm node ls-remote --lts # 查看已安装版本 xvm node list # 查看默认版本 xvm node default 安装指定版本\n# 安装指定版本 xvm node install v18.19.1 # 安装指定版本，并设置为默认版本, 重启terminal或者source生效 xvm node install v18.19.1 --default 卸载指定版本\nxvm node uninstall v18.19.1 切换版本\n# 临时切换指定版本 xvm node use v18.19.1 # 临时切换指定版本，并设置为默认 xvm node use v18.19.1 --default Go 查看版本\nxvm go ls-remote xvm go list 安装指定版本\nxvm go install go1.19.2 # 安装并设置为默认版本 xvm go install go1.19.2 --default # xvm go install go1.13.10 --default --arch=amd64 如果需要指定下载地址，可以在 ~/.xvm/scripts/go-scripts/proxy 文件中指定，如 https://golang.google.cn\n卸载指定版本\nxvm go uninstall go1.19.2 切换版本\n# 临时切换指定版本 xvm go use go1.19.2 # 临时切换指定版本，并设置为默认 xvm go use go1.19.2 --default 查看默认版本\nxvm go default Python xvm 也支持管理虚拟环境，并且操作很简单\n安装依赖\nPython使用的是源码编译安装，需要安装依赖，请参考 https://devguide.python.org/getting-started/setup-building/\n查看版本\nxvm python ls-remote xvm python list 安装指定版本\nxvm python install 3.12.0 # 安装并设置为默认版本 xvm python install 3.12.0 --default 切换版本\n# 临时切换指定版本 xvm python use 3.12.0 # 临时切换指定版本，并设置为默认 xvm python use 3.12.0 --default 查看默认版本\nxvm python default 创建虚拟环境\nxvm python alias 3.12.0 venv 激活虚拟环境\nxvm python activate venv 撤销虚拟环境\nxvm python deactivate 卸载指定版本\nxvm python uninstall 3.12.0 # 删除虚拟环境 xvm python uninstall venvxVM：Node.js、Go、Python版本管理器 ","permalink":"https://blog.crazykids.tech/zh/posts/xvm/","summary":"作为一名全干工程师，我们会在同一个电脑上安装各种工具，然而又会因为各种原因，需要使用这些工具的不同版本，版本管理就变的十分重要。 我们会使用 nvm 来管理 Node.js； 我们会使用 gvm 来管理 Go； 我们会使用 pyenv 或者 Conda 来管理 Python； 这些工具都十分强大，但使用方式却各不相同，一段时间不使用","title":"XVM：Node.js、Go、Python版本管理器X Version Manger"},{"content":"欢迎访问 Blackscreen.space，这里以黄色的温暖与活力为核心。我们深知黄色的心理意义，它象征幸福、阳光与积极正能量。Blackscreen.space 致力于打造以黄屏为核心的多功能平台，从提升积极情绪与能量到优化视觉清晰度，为您的数字体验带来全面升级。\n黄屏的积极与活力特质 光明与乐观的象征 黄色代表幸福与阳光，传递出明亮、轻快与乐观的气息。在 Blackscreen.space 上使用黄屏，能迅速提升积极氛围，营造充满活力的数字空间。其丰富的黄色调不仅振奋人心，还为观众带来愉悦的视觉享受。\n亲切与友好的魅力 黄色以其可爱与亲和力闻名，常用于食品包装与各类产品设计，深受男女喜爱。通过 Blackscreen.space 的黄屏，您可以为数字形象注入友好、平易近人且愉悦的特质，增强用户好感度。\n黄屏助力视觉沟通 打造吸睛的媒体广告 黄色象征温暖、欢乐与乐观，在不同光线与场景下传递多样情感。全球众多知名品牌在食品、家电、能源与科技领域采用黄色标识。借助 Blackscreen.space 的黄屏，制作引人入胜的广告内容，激发积极情绪，与观众建立深层共鸣。\n突出折扣与吸引目光 黄色因其活力与动态感，成为吸引注意力的绝佳选择。在 Blackscreen.space 上利用黄屏，您可以轻松突出折扣、优惠或关键信息，打造充满生机的数字环境，吸引并留住访客。\n黄屏带来的健康与舒适 护眼与舒适的显示效果 相较于蓝光的潜在负面影响，黄屏提供更健康的选择。Blackscreen.space 的温暖黄色光芒能缓解长时间使用电子设备带来的不适与眼疲劳，提升视觉舒适度，呵护眼睛健康。\n柔和阅读与放松照明 黄光是阅读或营造放松氛围的理想光源。与增强专注的白光不同，Blackscreen.space 的黄屏带来舒缓效果，减轻眼部压力。沉浸在温馨的光芒中，享受阅读或休闲时光。\n黄屏激发创意表达 摄影中的黄光创意 黄光常为摄影增添怀旧与历史感，营造独特氛围。通过 Blackscreen.space 的黄屏，您可以尝试不同角度与光线，拍摄出令人惊艳的肖像与视觉作品，唤起永恒与魅力的意境。\n","permalink":"https://blog.crazykids.tech/zh/posts/yellow-screen/","summary":"欢迎访问 Blackscreen.space，这里以黄色的温暖与活力为核心。我们深知黄色的心理意义，它象征幸福、阳光与积极正能量。Blackscreen.space 致力于打造以黄屏为核心的多功能平台，从提升积极情绪与能量到优化视觉清晰度，为您的数字体验带来全面升级。 黄屏的积极与活力","title":"Yellow Screen"},{"content":"欢迎访问 Blackscreen.space，这里以红色的动态与迷人魅力为核心。我们深知红色是一种引人注目的色彩，承载着丰富的含义与联想。Blackscreen.space 致力于打造以红屏为核心的多功能平台，从激发强烈情感与活力到优化视觉效果，为您的数字生活注入无限可能。\n红屏的心理魅力 引人注目与震撼力 红色向来以其吸睛的特质著称，是一种强烈的视觉刺激物。它的心理意义丰富多样，在不同文化中象征着力量、高贵、愤怒、激情乃至幸福。红色的多重解读使其成为激发情感与传递文化内涵的独特工具。\n激发热情与活力 红色以激发热情与能量著称，能够唤起欣快、活力甚至警觉的情绪。它让人感到温暖而充满动力。借助 Blackscreen.space 的红屏功能，您可以打造引人入胜且充满活力的视觉体验。\n红屏的多样应用 媒体与广告中的红屏效应 红屏散发温暖与高能量，是媒体与广告的理想选择。不同强度的红色可唤起激情、危险、冒险或诱惑等情感。Blackscreen.space 让您灵活运用红色传递品牌信息，在食品行业激发食欲，或在科技与工程领域展现热情与速度，吸引观众目光。\n提升视频通话的红屏魅力 在视频会议中，红屏能营造独特而深刻的氛围。结合红光散射与红色服饰，您可以传递温暖并留下难忘印象。Blackscreen.space 的红屏与环境光搭配，提升面部吸引力，为通话增添积极与乐观的气息。\n营造戏剧化氛围 无论是沉浸于悬疑小说，还是与朋友共创恐怖派对，红屏都能带来戏剧化的沉浸感。Blackscreen.space 的红屏替代刺眼灯光，增强环境氛围，让体验更具张力与吸引力。让红色为恐怖故事或游戏增添悬念与刺激。\n","permalink":"https://blog.crazykids.tech/zh/posts/red-screen/","summary":"欢迎访问 Blackscreen.space，这里以红色的动态与迷人魅力为核心。我们深知红色是一种引人注目的色彩，承载着丰富的含义与联想。Blackscreen.space 致力于打造以红屏为核心的多功能平台，从激发强烈情感与活力到优化视觉效果，为您的数字生活注入无限可能。 红屏的心理","title":"Red Screen"},{"content":"绿屏的舒缓与放松魅力 连接自然之美 绿色是大自然的象征，对我们的内心平静与纯净感有着深远影响。在融入绿色元素的环境中，人们往往感到更加放松与自在。公共场所、餐厅和酒店常通过设计、家具及植物引入绿色元素。借助 Blackscreen.space 的绿屏，您可以将自然的舒缓气息融入数字空间。\n减压与提升专注力 凝视绿色——无论是置身自然、浏览照片，还是使用鲜艳的绿屏——都能带来平静与放松的效果。绿色与宁静紧密相连，有助于缓解压力、增强自控力并提升专注度。通过绿屏的力量，打造一个宁静且高效的数字环境。\n绿屏的实用影响力 打造引人注目的广告牌 色彩在吸引客户与塑造品牌形象中至关重要。精心设计的绿屏广告牌能激发情感、抓住眼球并留下深刻印象。绿色传递共鸣、专业性与信任感，利用 Blackscreen.space 的绿屏工具，最大化您的广告视觉效果。\n提升影视与摄影效果 绿屏技术在电影制作与摄影领域广受欢迎。Blackscreen.space 的绿色背景便于后期移除并替换为多样化场景，为创作者提供无限灵活性。释放想象力，利用绿屏的多功能性，打造视觉惊艳的影片与照片。\n革新图像与视频内容 绿屏是内容创作的强大助力。通过 Blackscreen.space，您可以轻松替换背景、添加特效或调整色彩，塑造独特视觉风格。让绿屏的魔力为您的图像与视频注入魅力，为观众带来难忘体验。\n","permalink":"https://blog.crazykids.tech/zh/posts/green-screen/","summary":"绿屏的舒缓与放松魅力 连接自然之美 绿色是大自然的象征，对我们的内心平静与纯净感有着深远影响。在融入绿色元素的环境中，人们往往感到更加放松与自在。公共场所、餐厅和酒店常通过设计、家具及植物引入绿色元素。借助 Blackscreen.space 的绿屏，您可以将自然的舒缓气息融入数字空间。 减压与提升专注力 凝视绿色——无论","title":"Green Screen"},{"content":"欢迎访问 Blackscreen.space，这里以宁静与灵感为核心。我们相信蓝色能够唤起平静、稳定与高效的感受。\nBlue Screen的宁静与激励力量 宁静、安全与秩序 蓝屏让人联想到宁静与平和的情感。蓝色常与安静、安全及秩序感相联系，传递出可靠与稳定的气质。这种颜色不仅令人安心，还能激发信任与自信。\n提升生产力与专注力 研究显示，蓝色环境有助于提升生产力，是办公空间设计的优选色彩。蓝屏营造的平静氛围能帮助您保持专注，释放最佳潜能。利用 Blackscreen.space 的蓝屏功能，打造高效的工作与学习空间。\n蓝屏的实用场景 营造宁静与积极氛围 蓝屏唤起天空、海洋与黄昏的意象，带来宁静与积极的情感。通过 Blackscreen.space，您可以享受蓝屏带来的舒缓体验，为数字生活增添一抹平和与放松。\n高效复制与图像分层 Blackscreen.space 的蓝屏技术让图像复制与分层变得简单高效。无论是艺术创作还是教育用途，蓝屏都能支持精准编辑与呈现，助您展现卓越技能，令人叹为观止。\n优化视觉内容创作 无需高昂成本或远离工作场所，您也能打造引人注目的视觉内容。Blackscreen.space 的专业蓝屏背景能提升品牌形象，强化信息传递，帮助您在竞争中脱颖而出，创作独具特色的视频内容。\n蓝屏助力在线交流 呈现专业形象 在线会议中，以最佳状态展示自己至关重要。Blackscreen.space 的蓝屏背景提供柔和均匀的光线，营造视觉吸引力十足的环境，避免分散注意力的阴影，让您展现专业与从容的风采。\n","permalink":"https://blog.crazykids.tech/zh/posts/blue-screen/","summary":"欢迎访问 Blackscreen.space，这里以宁静与灵感为核心。我们相信蓝色能够唤起平静、稳定与高效的感受。 Blue Screen的宁静与激励力量 宁静、安全与秩序 蓝屏让人联想到宁静与平和的情感。蓝色常与安静、安全及秩序感相联系，传递出可靠与稳定的气质。这种颜色不仅令人安心，还能激发信","title":"Blue Screen"},{"content":"被工作追着跑，吃饭选择别再烧脑！“饭点儿”APP来救场\n身为上班族，每天忙碌于工作，本就神经紧绷。可一到中午、晚上这些饭点儿，又要被“吃什么”这个难题折磨。打开外卖软件，菜品繁多，刷来刷去，眼睛都花了，还是毫无头绪，宝贵的午休时间就这样在纠结中流逝。出去觅食吧，面对街边密密麻麻的店铺，选择困难症瞬间发作，饥肠辘辘却无从下手。这种纠结不定的感觉，是不是让你头疼不已？\n别担心，“饭点儿”APP就是你的救星，专为广大上班族量身打造，轻松解决吃饭选择难题。\n“饭点儿”最大的亮点，就是把选择的难题变成有趣的探索。无需在众多选项中苦苦挣扎，只需轻松摇一摇手机，一款附近的优质餐馆就会出现在你眼前。这种简单又充满趣味的交互方式，没有复杂的操作门槛，人人都能快速上手，让找餐馆变得像玩游戏一样轻松。\n每次摇一摇，都是一次新的惊喜。“饭点儿”会精准提供推荐餐馆的距离和具体位置，还有多角度的饭馆实景照片，让你还没到店，就能对餐馆有直观了解，提前感受用餐氛围。无论你是想找一家环境优雅的西餐厅犒劳自己，还是寻觅一家烟火气十足的小饭馆品尝家乡味道，“饭点儿”都能满足你。\n别再让吃饭选择成为负担，下载“饭点儿”APP ，让摇一摇开启美味新体验，从此告别饭点儿纠结，轻松享受每一餐！\n","permalink":"https://blog.crazykids.tech/zh/posts/fan-dian-er/","summary":"被工作追着跑，吃饭选择别再烧脑！“饭点儿”APP来救场 身为上班族，每天忙碌于工作，本就神经紧绷。可一到中午、晚上这些饭点儿，又要被“吃什么”这个难题折磨。打开外卖软件，菜品繁多，刷来刷去，眼睛都花了，还是毫无头绪，宝贵的午休时间就这样在纠结中流逝。出去觅食吧，面对街边密密麻麻的店","title":"饭点儿"},{"content":"欢迎访问 Blackscreen.space，这里以纯净而耀眼的白色为核心。我们深信，简洁与清晰是塑造卓越视觉体验的基础。\n白屏的力量 纯净清晰的视觉体验 Blackscreen.space 为您呈现白屏的极致美感。作为一块纯净的数字画布，它让您的图像与照片以惊艳的清晰度脱颖而出。无论是复制图片还是开启全新设计，White Screen都是追求精确与创意的理想选择，提升您的视觉呈现效果。\n优化的照明与平衡 通过 Blackscreen.space，您可以轻松获得理想的照明条件。无论是人像摄影还是专业工作室搭建，白屏作为可靠的光源，确保光线均匀柔和。其明亮的白色特性为您的创意项目增添光彩与深度，助力打造出色的视觉构图。\n白屏的实用价值 检测屏幕死点与优化触摸性能 白屏在排查屏幕问题时表现尤为出色，尤其适用于触摸设备。Blackscreen.space 的纯白背景能清晰揭示任何无响应区域，简化故障诊断与导航过程，帮助您快速提升设备的触摸体验与性能。\n多场景日常应用 白屏不仅限于摄影与数字界面，还能无缝融入日常生活。Blackscreen.space 可作为便携光源，为弱光环境下的阅读或物品查找提供便利；亦可化身化妆灯，带来聚焦照明以确保精准妆容。其明亮背景还能助力清洁，让污渍与异物一览无余。\n激发无限创意 以白屏为起点，释放您的创造力。Blackscreen.space 为动画艺术与故事叙述提供完美平台。只需白纸与笔等简单工具，您即可绘制引人入胜的插图与动画。借助白色的纯粹，将您的想象力转化为生动现实。\n","permalink":"https://blog.crazykids.tech/zh/posts/whitescreen/","summary":"欢迎访问 Blackscreen.space，这里以纯净而耀眼的白色为核心。我们深信，简洁与清晰是塑造卓越视觉体验的基础。 白屏的力量 纯净清晰的视觉体验 Blackscreen.space 为您呈现白屏的极致美感。作为一块纯净的数字画布，它让您的图像与照片以惊艳的清晰度脱颖而出。无论是复制图片还是开启全新设计，Whit","title":"White Screen"},{"content":"Blackscreen.space 是您探索黑色魅力的终极之选。我们坚信，简约与优雅是打造引人入胜视觉体验的核心所在。\nBlack Screen 的多功能性 无需关闭电源即可调节亮度 Black Screen 的核心功能之一在于，它能够在不完全关闭第二台显示器的情况下实现屏幕变暗。这一特性尤为适合需要在专注游戏或沉浸式电影体验中快速关闭辅助显示器的场景。Black Screen 让您轻松实现屏幕暗化，同时确保视觉沉浸感不受干扰。\n节能环保的明智选择 为响应可持续发展的号召，Blackscreen.space 通过启用黑色屏幕有效降低能耗并减少碳足迹。当您的计算机处于闲置状态时，选择黑屏模式不仅能节约能源，还能为环境保护贡献一份力量。通过简单而有意义的举措，我们携手为地球带来积极改变。\nBlack Screen 的多样化应用 隐秘您的 PC 活动 需要一种低调的方式隐藏计算机运行状态？Blackscreen.space 提供了一个优雅的解决方案。在多显示器环境中，当您希望屏幕保持开启而又不引人注意时，这一功能尤为实用。借助 Black Screen，您可以兼顾隐私保护与工作效率的优化。\n提升屏幕清洁效率 对于追求屏幕洁净的用户，Blackscreen.space 是一个实用的辅助工具。通过显示纯黑背景，即使是最细微的污渍或指纹也会一览无余，从而显著提升清洁效果。只需切换至 Black Screen 模式，您即可轻松确保显示器一尘不染，始终呈现最佳视觉效果。\n游戏与设计的理想搭档 对于游戏玩家和设计师而言，Blackscreen.space 是不可或缺的创作伙伴。其深邃的黑色背景能够增强对比度，将注意力集中于您的内容之上，让您完全沉浸于虚拟世界或设计作品中。您还可以自定义屏幕尺寸和边框颜色，使创作在优雅的黑色画布上绽放光彩。\nBlack Screen：全方位的解决方案 提升专注力与学习效率 在学习过程中保持专注往往充满挑战，尤其是在长时间面对明亮屏幕导致眼部疲劳时。Blackscreen.space 通过提供舒缓的视觉环境，帮助减少干扰并提升专注力。借助其宁静且放松的氛围，您可以更轻松地实现学习目标，优化学习体验。\n眼部舒适与视觉放松 在数字时代，我们的眼睛承受着前所未有的压力。Blackscreen.space 以其柔和的黑色调有效缓解眼部疲劳，为用户提供舒适的视觉体验。无论是长时间工作还是娱乐，Blackscreen.space 都能为您带来持久的视觉享受。\n操作方式 全屏 双击可以进入全屏模式，再双击退出全屏模式\n颜色切换 按鼠标方向键可以在 black screen, white screen, red screen, green screen, blue screen, yellow screen 之间循环切换\n","permalink":"https://blog.crazykids.tech/zh/posts/blackscreen/","summary":"Blackscreen.space 是您探索黑色魅力的终极之选。我们坚信，简约与优雅是打造引人入胜视觉体验的核心所在。 Black Screen 的多功能性 无需关闭电源即可调节亮度 Black Screen 的核心功能之一在于，它能够在不完全关闭第二台显示器的情况下实现屏幕变暗。这一特性尤为适合需要在专注游戏或沉浸式电影体验中快速关闭辅助显示器的场景。Blac","title":"Black Screen"},{"content":"git 仓库迁移，一般分为两种情况，从仓库 A 迁移到仓库 B 和从本地直接上传到仓库 B。\n从仓库 A 迁移到仓库 B 这种方式比较常见，也有很多 web 端直接支持镜像的方式，但批量的迁移还是要使用命令的，如下：\n$ git clone --bare http://a.com/crazykids.git $ git push --mirror http://b.com/crazykids.git 从本地直接上传到仓库 B 有的时候需要将本地仓库直接全部 push 到仓库 B，比如仓库 A 真的的挂了，或者你有多仓库备份的习惯。一般 push 命令只能将当前 branch 或者已经 checkout 出来的分支 push 的远端。\n增加仓库 B 地址 $ git remote add B http://b.com/crazykids.git 方式1 checkout 所有分支，再push到远端\n$ git fetch origin $ git branch -r | grep origin | grep -v HEAD|while read rb;do lb=$(echo ${rb}|cut -d/ -f 2-);git checkout -b $lb $rb;done $ git branch -r | grep origin | grep -v HEAD|while read rb;do lb=$(echo ${rb}|cut -d/ -f 2-);git push -f B $lb;done $ git push --tags 不但命令麻烦，如果遇到分支差异比较大的，还经常出现问题。\n方式2 $ git fetch origin $ git push B \u0026#39;refs/remotes/origin/*\u0026#39;:\u0026#39;refs/heads/*\u0026#39; \u0026#39;refs/tags/*\u0026#39;:\u0026#39;refs/tags/*\u0026#39; ","permalink":"https://blog.crazykids.tech/zh/posts/git-mirgration/","summary":"git 仓库迁移，一般分为两种情况，从仓库 A 迁移到仓库 B 和从本地直接上传到仓库 B。 从仓库 A 迁移到仓库 B 这种方式比较常见，也有很多 web 端直接支持镜像的方式，但批量的迁移还是要使用命令的，如下： $ git clone --bare http://a.com/crazykids.git $ git push --mirror http://b.com/crazykids.git 从本地直接上传到仓库 B 有的时候需要将本地仓库直接全部 push 到仓库 B，比如仓库 A 真的","title":"Git 仓库迁移"},{"content":"查看所有分支 $ git branch -a 查看远程分支 $ git branch -r 查看本地分支所关联的远程分支 $ git branch -vv 重命名本地分支 $ git branch -m old_branch new_branch 删除远程分支 $ git branch -d -r origin/old_branch $ git push origin :old_branch 推送新的分支 $ git push -u origin new_branch 设置关联 $ git branch --set-upstream-to=origin/branch branch ","permalink":"https://blog.crazykids.tech/zh/posts/git_branch/","summary":"查看所有分支 $ git branch -a 查看远程分支 $ git branch -r 查看本地分支所关联的远程分支 $ git branch -vv 重命名本地分支 $ git branch -m old_branch new_branch 删除远程分支 $ git branch -d -r origin/old_branch $ git push origin :old_branch 推送新的分支 $ git push -u origin new_branch 设置关联 $ git branch --set-upstream-to=origin/branch branch","title":"git 分支操作常用命令"},{"content":"00 安装Hugo 到 Hugo Releases 下载对应的操作系统版本的 Hugo 二进制文件\nMac 下可以直接使用 Homebrew 安装：\n$ brew install hugo 01 生成站点 使用 Hugo 快速生成站点\n$ hugo new site /path/to/site 这样就在 /path/to/site 目录里生成了初始站点，进入目录：\n$ cd /path/to/site 02 安装主题 到 主题列表 挑选主题，找到相关的 GitHub 地址，创建目录 themes, 把主题 clone 下来\n$ git clone https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 在配置文件中设置主题名字\ntheme: PaperMod 03 配置文件 Hugo 默认使用的 toml 格式的配置文件，也支持 yaml 格式。 详细说明可以参考 Hugo的官方文档\n示例配置 config.yaml 如下\nbaseURL: \u0026#39;https://blog.crazykids.tech/\u0026#39; theme: \u0026#39;PaperMod\u0026#39; languageCode: \u0026#39;zh-cn\u0026#39; languageName: \u0026#39;简体中文\u0026#39; hasCJKLanguage: true title: \u0026#39;TimeStamps\u0026#39; paginate: 15 summaryLength: 120 # 文章概览的自字数，默认70 author: name: \u0026#39;duanhongjin\u0026#39; email: \u0026#39;duanhjlt@163.com\u0026#39; link: \u0026#39;https://blog.crazykids.tech/\u0026#39; enableInlineShortcodes: true enableEmoji: true enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false googleAnalytics: UA-123-45 minify: disableXML: true # minifyOutput: true menu: main: - weight: 1 identifier: \u0026#39;post\u0026#39; name: \u0026#39;文章\u0026#39; url: \u0026#39;/\u0026#39; - weight: 2 identifier: \u0026#39;tags\u0026#39; name: \u0026#39;标签\u0026#39; url: \u0026#39;/tags/\u0026#39; - weight: 3 identifier: \u0026#39;categories\u0026#39; name: \u0026#39;分类\u0026#39; url: \u0026#39;/categories/\u0026#39; - weight: 4 identifier: \u0026#39;about\u0026#39; name: \u0026#39;关于\u0026#39; url: \u0026#39;/about/\u0026#39; outputs: home: - HTML - RSS - JSON params: env: production # to enable google analytics, opengraph, twitter-cards and schema. author: duanhongjin defaultTheme: auto # defaultTheme: light or dark disableThemeToggle: false DateFormat: \u0026#34;2006-01-02\u0026#34; ShowShareButtons: true ShowReadingTime: true # disableSpecialistPost: true displayFullLangName: true ShowPostNavLinks: true ShowBreadCrumbs: true ShowCodeCopyButtons: true hideFooter: false # 隐藏页脚 ShowWordCounts: true VisitCount: true ShowLastMod: true #显示文章更新时间 ShowToc: true # 显示目录 TocOpen: true # 自动展开目录 extendCodeBackground: false # 代码块是否自动横向展开 comments: true socialIcons: - name: github url: \u0026#34;https://github.com/duanhjlt\u0026#34; - name: RSS url: \u0026#34;index.xml\u0026#34; cover: hidden: false # hide everywhere but not in structured data hiddenInList: false # hide on list pages and home hiddenInSingle: false # hide on single page editPost: URL: \u0026#34;https://github.com/duanhjlt/blog/tree/main/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link markup: highlight: noClasses: false 04 启动博客 $ hugo server -D 本地预览，本地预览网址为 http://localhost:1313\n$ hugo -F --cleanDestinationDir 生成全新的 public 文件夹，这个文件夹可以部署到云服务器上\n05 写文章 $ hugo new new-post.md hugo 会在 content 目录下生成\u0026quot;new-post.md\u0026quot;名字的文件，所有的文章都会放到 content 目录下\n如何自己定义的子目录，如 posts，可以使用命令\n$ hugo new posts/new-post.md 06 文章默认配置 生成文章内部文件头部配置信息包括一些文章名称，时间之类的信息，可以写到 archetypes/default.md 中做为模版，这样生成文章时会自动添加模版里的配置\n示例配置如下:\n--- keywords: [] title: \u0026#39;{{ replace .File.ContentBaseName \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#39; date: {{ .Date }} lastmod: {{ .Date }} draft: false description: author: \u0026#39;duanhongjin\u0026#39; tags: [] categories: [] tocLevels: [\u0026#34;h2\u0026#34;, \u0026#34;h3\u0026#34;, \u0026#34;h4\u0026#34;] comments: true # 本页面是否显示评论 reward: true # 打赏 showToc: true # 显示目录 TocOpen: true # 自动展开目录 hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等 disableShare: true # 底部不显示分享栏 showbreadcrumbs: true #顶部显示路径 --- 07 部署到GitHub Pages 在 GitHub 上创建仓库，名字可以是 duanhjlt.github.io，也可以是其他的，在 GitHub Pages 进行设置即可\n如果想使用自己的域名，如本站域名 blog.crazykids.tech，可以\n在 dns 增加 blog 到 duanhjlt.gihub.io. 的 CNAME 配置项 在 GitHub 的仓库中增加 CNAME 文件，内容为 blog.crazykids.tech 将 public 文件夹做为 GitHub Pages 的本地仓库，每次生成后，推送到 GitHub 上，GitHub Pages 就会自动部署了\n","permalink":"https://blog.crazykids.tech/zh/posts/hugo-site/","summary":"00 安装Hugo 到 Hugo Releases 下载对应的操作系统版本的 Hugo 二进制文件 Mac 下可以直接使用 Homebrew 安装： $ brew install hugo 01 生成站点 使用 Hugo 快速生成站点 $ hugo new site /path/to/site 这样就在 /path/to/site 目录里生成了初始站点，进入目录： $ cd /path/to/site 02 安装主题 到 主题列表 挑选主题，找到相关的 GitHub 地址，创建目录 themes, 把主题 clone 下来 $ git clone https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 在配置文件中设置主题名字 theme:","title":"Hugo + Github Pages 建站"},{"content":"中年 iOS/macos 开发\nMemora：专为iOS和macOS设计的离线稍后读应用: Memora是一款安全可靠的稍后读应用，支持网页离线保存、iCloud同步和无干扰阅读体验。通过标签分类和智能搜索，轻松管理你的阅读清单。 Black Screen: Black Screen - Fullscreen Focus, OLED \u0026amp; Pixel Testing xvm: [node|golang|python] 版本管理 xvm-windows: [node|golang|python] 版本管理 饭点儿: 到饭点儿，轻轻一摇推荐附近的餐馆 It is friday?: Check if it is friday? Twitter Video Downloader | TwitterXVideo: 下载、编辑、翻译和重新发布 — 一站式搞定。免费且无需注册。 乘风客的分享: 持续分享好用的效率工具、AI、实用网站、爆款应用、学习资料、影视视频等。 Brat Generator | Brat-Gen: Free Brat Generator for Custom Covers Unix Timestamp Converter: Unix Timestamp - Free Unix Timestamp Converter \u0026amp; Date Tool String Art Generator: Free Online Pattern Creator Retro Image Prompt: 用 Nano Banana 一键生成复古风格图片 联系方式 E-mail: duanhjlt@gmail.com Github: duan0120 X: @duanhjlt Telegram: @duanhjlt bento wulalatalk\n","permalink":"https://blog.crazykids.tech/zh/about/","summary":"中年 iOS/macos 开发 Memora：专为iOS和macOS设计的离线稍后读应用: Memora是一款安全可靠的稍后读应用，支持网页离线保存、iCloud同步和无干扰阅读体验。通过标签分类和智能搜索，轻松管理你的阅读清单。 Black Screen: Black Screen - Fullscreen Focus, OLED \u0026amp; Pixel Testing xvm: [node|golang|python] 版本管理 xvm-windows: [node|golang|python] 版本管理 饭点儿: 到饭点儿，轻轻一摇推","title":"简介"},{"content":"1. 引言 欢迎访问 TimeStamps（以下简称”本网站”）。我们重视您的隐私，并致力于保护您的个人信息。本隐私政策旨在说明我们如何收集、使用和保护您的信息。\n2. 信息收集 2.1 自动收集的信息 我们可能会自动收集以下信息：\n访问日志（IP地址、浏览器类型、访问时间）\n设备信息（操作系统、设备类型）\n浏览行为（访问页面、停留时间） ###2.2 Cookie 使用 本网站使用 Cookie 和类似技术来：\n记住您的偏好设置\n分析网站流量\n改善用户体验\n3. 信息使用 我们收集的信息将用于：\n提供和改进网站服务 分析网站使用情况 优化网站内容和用户体验 防止欺诈和滥用 4. 信息共享 我们不会出售、出租或以其他方式分享您的个人信息，除非：\n获得您的明确同意 法律要求 保护网站及用户权益 5. 第三方服务 本网站使用以下第三方服务：\nGoogle Analytics（网站分析） Google AdSense（广告服务） 这些服务可能收集您的信息，建议您查看它们各自的隐私政策。 6. 数据安全 我们采取适当的技术和组织措施来保护您的信息，防止未经授权的访问、使用或泄露。\n7. 您的权利 您有权：\n访问您的个人信息 更正不准确的信息 要求删除您的信息 反对或限制信息处理 撤回同意 儿童隐私 本网站不面向13岁以下儿童，我们不会故意收集儿童的个人信息。\n隐私政策更新 我们可能会不时更新本隐私政策。更新后的政策将在本页面上发布，并注明最后更新日期。\n联系我们 如果您对本隐私政策有任何问题或建议，请通过以下方式联系我们：\n电子邮件：duanhjlt@gmail.com 适用法律 本隐私政策受中华人民共和国法律管辖。 ","permalink":"https://blog.crazykids.tech/zh/privacy/","summary":"1. 引言 欢迎访问 TimeStamps（以下简称”本网站”）。我们重视您的隐私，并致力于保护您的个人信息。本隐私政策旨在说明我们如何收集、使用和保护您的信息。 2. 信息收集 2.1 自动收集的信息 我们可能会自动收集以下信息： 访问日志（IP地址、浏览器类型、访问时间） 设备信息（操作系统、设备类型）","title":"隐私政策"}]